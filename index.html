<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0041)https://people.eecs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	<style type="text/css">
		/* Color scheme stolen from Sergey Karayev */
		a {
		color: #1772d0;
		text-decoration:none;
		}
		a:focus, a:hover {
		color: #f09228;
		text-decoration:none;
		}
		body,td,th,tr,p,a {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px
		}
		ps {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px
		}
		strong {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		red {
		color: #d02717;
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		}
		grey {
		color: #696969;
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		heading {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 22px;
		}
		px14 {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		awardtitle {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 18px;
		/*font-weight: 700*/
		}
		papertitle {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		font-weight: 700
		}
		name {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 32px;
		}
		.one
		{
		width: 160px;
		height: 160px;
		position: relative;
		}
		.two
		{
		width: 160px;
		height: 160px;
		position: absolute;
		transition: opacity .2s ease-in-out;
		-moz-transition: opacity .2s ease-in-out;
		-webkit-transition: opacity .2s ease-in-out;
		}
		.fade {
		 transition: opacity .2s ease-in-out;
		 -moz-transition: opacity .2s ease-in-out;
		 -webkit-transition: opacity .2s ease-in-out;
		}
		span.highlight {
				background-color: #ffffd0;
		}
	</style>
	<!--<link rel="icon" type="image/png" href="https://people.eecs.berkeley.edu/~barron/seal_icon.png">-->
	<title>Zhengyuan Yang</title>
	
	<link href="./index_files/css" rel="stylesheet" type="text/css">
	</head>
	<body>
	<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
		<tbody><tr>
		<td>
			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td width="67%" valign="middle">
				<p align="center">
					<name>Zhengyuan Yang</name>
				</p>
				<p>I am currently a <a href="https://www.microsoft.com/en-us/research/project/azure-florence-vision-and-language/">Senior Researcher at Microsoft</a>. I received my Ph.D. degree in Computer Science at <a href="http://www.cs.rochester.edu/">University of Rochester</a>, advised by <a href="http://www.cs.rochester.edu/u/jluo/#VISTA">Prof. Jiebo Luo</a>. I did my bachelors at <a href="http://en.sist.ustc.edu.cn/">University of Science and Technology of China</a>. I've received <a href="http://www.sigmm.org/Awards/thesisaward">ACM SIGMM Award for Outstanding Ph.D. Thesis</a>, <a href="https://blog.twitch.tv/en/2020/01/15/introducing-our-2020-twitch-research-fellows/"> Twitch Research Fellowship</a>, and <a href="./BIRPA.JPG"> ICPR 2018 Best Industry Related Paper Award</a>. My research interests involve the intersection of computer vision and natural language processing, including multi-modal vision-language understanding and generation.
				<br>
				<br>
				<br>
				<p align="center">
<a href="mailto:zhengyuan.yang13@gmail.com">Email</a> &nbsp;/&nbsp;
<a href="./resume_Zhengyuan_Yang.pdf">CV</a> &nbsp;/&nbsp;
<a href="https://github.com/zyang-ur">Github</a> &nbsp;/&nbsp;
<a href="https://scholar.google.com/citations?user=rP02ve8AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
<a href="https://www.linkedin.com/in/zhengyuan-yang-992b52105"> LinkedIn </a> &nbsp;/&nbsp;
<a href="https://translate.google.com/?sl=zh-CN&tl=en&text=%E6%9D%A8%20%E5%BE%81%E5%85%83&op=translate"> Name Pronounce </a>

				</p>
				</td>
				<td width="37%">
				<!--<img src="./index_files/zyang2.jpg">-->
<!-- 	<img src="./zyang2.jpg"> -->
	<!-- <img src="./zyang2.jpg"> -->
<img src='./zyang2.jpg' onmouseover="this.src='./IMG_3086.jpg';" onmouseout="this.src='./zyang2.jpg';" />
				</td>
			</tr>
			</tbody></table>


<!-- News -->

      <!-- <br> -->
      <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-news" id="news"><heading>News</heading></button>
      <!-- <div class="container"> -->
      <div id="content-news" class="collapse in">
      <!-- <div class="scroll">  -->

      <table border=0 class="bg_colour" style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
     
<!--           <tr>
              <td><p style="color:darkblue; display:inline">June '21 &nbsp</p></td>
              <td>We are the winner of <a href="https://eval.ai/web/challenges/challenge-page/906/overview">TextCaps Challenge 2021</a> and <a href="https://referit3d.github.io/benchmarks.html">ReferIt3D Challenge 2021</a>. Welcome to check the related <a href="https://arxiv.org/pdf/2012.04638.pdf">TAP</a> and <a href="https://arxiv.org/pdf/2105.11450.pdf">SAT</a> papers.</td>
          </tr>
          <tr>
              <td><p style="color:darkblue; display:inline">June '21 &nbsp</p></td>
              <td>I defensed my Ph.D. dissertation and will join Microsoft as a Researcher.</td>
          </tr>
          <tr>
              <td><p style="color:darkblue; display:inline">May '21 &nbsp</p></td>
              <td>I am selected as one of <a href="http://cvpr2021.thecvf.com/node/184">Outstanding Reviewer</a> for CVPR 2021.</td>
          </tr>
          <tr>
              <td><p style="color:darkblue; display:inline">Feb '21 &nbsp</p></td>
              <td>Two papers accepted to CVPR 2021 (The <a href="https://github.com/microsoft/TAP">TAP</a> paper was selected as Oral).</td>
          </tr>
 -->
 				<ps>
 					<li> <px14>[2023/10]</px14> &nbsp How can LMM-based agents achieve human-like multimodal iterative exploration? Checkout our initial study on <a href="https://idea2img.github.io/">a generative agent, named Idea2Img<img src="./icon.png" alt="Idea2Img" width="20"/>,</a> focusing on automatic image design and generation.
 					<li> <px14>[2023/09]</px14> &nbsp What are the current state and promising future directions for large multimodal models (LMMs)? Please checkout our Preliminary Explorations with GPT-4V(ision): <a href="https://arxiv.org/abs/2309.17421">The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)</a>.
					<li> <px14>[2023/09]</px14> &nbsp Please checkout our survey paper/book on <a href="https://arxiv.org/abs/2309.10020">Multimodal Foundation Models: From Specialists to General-Purpose Assistants</a>. [<a href="https://vlp-tutorial.github.io/">Slides</a>] [<a href="https://www.youtube.com/playlist?list=PLB1k029in3UhWaAsXP1DGq8qEpWxW0QyS">YouTube</a>] [<a href="https://space.bilibili.com/20119746/channel/collectiondetail?sid=1480989">Bilibili</a>]
					<li> <px14>[2023/08]</px14> &nbsp <a href="https://github.com/yuweihao/MM-Vet">MM-Vet</a> is an LMM evaluation benchmark that evaluates Large Multimodal Models' integrated VL capabilities. [<a href="https://paperswithcode.com/sota/visual-question-answering-on-mm-vet?tag_filter=0">MM-Vet Leaderbaord</a>]
					<li> <px14>[2023/07]</px14> &nbsp Two papers accepted to ICCV 2023: (1) <a href="https://yushi-hu.github.io/promptcap_demo/">PromptCap</a>, prompt controlled visual captioning; (2) <a href="https://github.com/Wangt-CN/EqBen">EQBen</a>, a new diagnostic VLM benchmark.
					<li> <px14>[2023/06]</px14> &nbsp I will serve as a SPC member for AAAI 2024.
					<li> <px14>[2023/06]</px14> &nbsp Check out our CVPR 2023 Tutorial on <a href="https://vlp-tutorial.github.io/">"Recent Advances in Vision Foundation Models"</a>. Slides and recordings availble.
					<li> <px14>[2023/03]</px14> &nbsp We build <a href="https://multimodal-react.github.io">MM-REACT</a>, a system paradigm that integrates LLMs with a pool of vision experts to achieve multimodal reasoning and action.
					<li> <px14>[2023/03]</px14> &nbsp <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em> special issue on <a href="https://ieee-cas.org/files/ieeecass/2023-03/Call%20for%20papers%20%28AIGC%29_final_0306.pdf">"AI-Generated Content for Multimedia."</a> Submission deadline: July 1st, 2023.
					<li> <px14>[2023/02]</px14> &nbsp <a href="https://arxiv.org/pdf/2211.15518.pdf">ReCo</a> is our new text-to-image model that allows the precise region control of input text queries, accepted to CVPR 2023. <a href="./reco/reco.png">See a teaser here.</a>
					<li> <px14>[2023/01]</px14> &nbsp <a href="https://arxiv.org/pdf/2210.09150.pdf">Prompting GPT-3 To Be Reliable</a> accepted to ICLR 2023.
					<li> <px14>[2022/10]</px14> &nbsp My Ph.D. thesis <a href="https://www.proquest.com/docview/2572612109">"Visual Grounding: Building Cross-Modal Visual-Text Alignment"</a> wins the <a href="http://www.sigmm.org/Awards/thesisaward">2022 ACM SIGMM Award for Outstanding Ph.D. Thesis</a>.
					<li> <px14>[2022/10]</px14> &nbsp I am selected as one of <a href="https://eccv2022.ecva.net/program/outstanding-reviewers">Outstanding Reviewers</a> for ECCV 2022.
					</li>
					<li> <px14>[2022/07]</px14> &nbsp <a href="https://arxiv.org/pdf/2111.12085.pdf">UniTAB</a> accepted to ECCV 2022 as an Oral presentation.
					<li> <px14>[2022/07]</px14> &nbsp I will serve as a SPC member for AAAI 2023.
					<li> <px14>[2022/06]</px14> &nbsp Check out our CVPR 2022 Tutorial on <a href="https://vlp-tutorial.github.io/">"Recent Advances in Vision-and-Language Pre-training"</a>. Slides and recordings availble.
					<li> <px14>[2022/05]</px14> &nbsp The new multimodal generative foundation model <a href="https://arxiv.org/pdf/2205.14100.pdf">Florence-GIT</a> achieves new sota across 12 image/video VL tasks, including the first human-parity on TextCaps. GIT achieves 88.79% ImageNet-1k accuracy using a generative scheme. <a href="./fl-git/teaser.png">See a teaser here.</a>
					<li> <px14>[2022/01]</px14> &nbsp I will serve as an Associate Editor for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IEEE TCSVT</a>.
					<li> <px14>[2021/09]</px14> &nbsp Can GPT-3 benefit multimodal tasks? We provide an empirical study of GPT-3 for knowledge-based VQA, <a href="https://arxiv.org/pdf/2109.05014.pdf">named PICa.</a> (Selected as Oral in AAAI 2022)
<!-- 					<li> [2021/07] &nbsp Two papers accepted to ICCV 2021 (The <a href="https://arxiv.org/pdf/2105.11450.pdf">SAT</a> paper was selected as Oral).
					<li> [2021/06] &nbsp We are the winner of <a href="https://eval.ai/web/challenges/challenge-page/906/overview">TextCaps Challenge 2021</a> and <a href="https://referit3d.github.io/benchmarks.html">ReferIt3D Challenge 2021</a>. Welcome to check the related <a href="https://arxiv.org/pdf/2012.04638.pdf">TAP</a> and <a href="https://arxiv.org/pdf/2105.11450.pdf">SAT</a> papers.
					</li>
					<li> [2021/06] &nbsp I defensed my Ph.D. dissertation <a href="https://www.proquest.com/docview/2572612109">"Visual Grounding: Building Cross-Modal Visual-Text Alignment"</a> and will join <a href="https://www.microsoft.com/en-us/research/project/azure-florence-vision-and-language/">Microsoft as a Researcher</a>.
					</li>
					<li> [2021/05] &nbsp I am selected as one of <a href="http://cvpr2021.thecvf.com/node/184">Outstanding Reviewers</a> for CVPR 2021.
					</li>
					<li> [2021/02] &nbsp Two papers accepted to CVPR 2021 (The <a href="https://github.com/microsoft/TAP">TAP</a> paper was selected as Oral). -->
					</li>
				</ps>


      </tbody></table>
      </div>
      <hr class="soft">
                


			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td width="100%" valign="middle">
					<heading>Research</heading>
					<p>
					My current research mainly focues on vision+language understanding and generation. 
					<!-- I've also worked on human-centered visual understanding, such as human action recognition and parsing. -->
					Representative works are <span class="highlight">highlighted</span>.
					</p>
				</td>
			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./mmvet_teaser.png">
				<img src="./mmvet.jpeg" width="200px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2308.02490.pdf">MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities</a></papertitle><br>
				Weihao Yu*, <strong>Zhengyuan Yang*</strong>, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang <br>
				Technical report <br>
				[<a href="https://arxiv.org/pdf/2308.02490.pdf">PDF</a>]
				[<a href="https://paperswithcode.com/sota/visual-question-answering-on-mm-vet?tag_filter=0">MM-Vet Leaderbaord</a>]
				[<a href="https://github.com/yuweihao/MM-Vet">Code</a>]
				[<a href="https://huggingface.co/spaces/whyu/MM-Vet_Evaluator">Demo</a>]
				[<a href="./bibtex/yu2023mmvet.txt">Bibtex</a>]
				<p><grey> MM-Vet</a> is an LMM evaluation benchmark that evaluates Large Multimodal Models' integrated VL capabilities. </grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./disco.png">
				<img src="./disco.png" width="200px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2307.00040.pdf">DisCo: Disentangled Control for Referring Human Dance Generation in Real World</a></papertitle><br>
				Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, <strong>Zhengyuan Yang</strong>, Hanwang Zhang, Zicheng Liu, Lijuan Wang <br>
				Technical report <br>
				[<a href="https://arxiv.org/pdf/2307.00040.pdf">PDF</a>]
				[<a href="https://disco-dance.github.io/">Project Page</a>]
				[<a href="https://github.com/Wangt-CN/DisCo">Code</a>]
				[<a href="https://5e42cfd7d54823fd8a.gradio.live/">Demo</a>]
				[<a href="./bibtex/wang2023disco.txt">Bibtex</a>]
				<p><grey> We propose DisCo for referring human dance generation, producing human dance images/videos with good faithfulness, generalizability, and compositionality. </grey></p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./mmreact.png">
				<img src="./mmreact.png" width="200px" height="100px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2303.11381.pdf">MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action</a></papertitle><br>
				<strong>Zhengyuan Yang*</strong>, Linjie Li*, Jianfeng Wang*, Kevin Lin*, Ehsan Azarnasab*, Faisal Ahmed*, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang <br>
				Technical report <br>
				[<a href="https://arxiv.org/pdf/2303.11381.pdf">PDF</a>]
				[<a href="https://multimodal-react.github.io/">Project Page</a>]
				[<a href="https://github.com/microsoft/MM-REACT">Code</a>]
				[<a href="https://huggingface.co/spaces/microsoft-cognitive-service/mm-react">Demo</a>]
				[<a href="./bibtex/yang2023mm.txt">Bibtex</a>]
				<p><grey> MM-REACT</a> is a system paradigm that integrates LLMs with a pool of vision experts to achieve multimodal reasoning and action. </grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./reco/promptcap.png">
				<img src="./reco/promptcap.png" width="200px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2211.09699.pdf">PromptCap: Prompt-Guided Task-Aware Image Captioning</a></papertitle><br>
				Yushi Hu, Hang Hua, <strong>Zhengyuan Yang</strong>, Weijia Shi, Noah A. Smith, Jiebo Luo <br>
				ICCV 2023. <br>
				[<a href="https://arxiv.org/pdf/2211.09699.pdf">PDF</a>]
				[<a href="https://yushi-hu.github.io/promptcap_demo/">Project Page</a>]
				[<a href="https://github.com/Yushi-Hu/PromptCap">Code</a>]
				[<a href="./bibtex/hu2022promptcap.txt">Bibtex</a>]
				<p><grey> PromptCap takes a natural language prompt to control the visual content to describe by the captioning model. </grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./eqben.png">
				<img src="./eqben.png" width="200px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2303.14465.pdf">Equivariant Similarity for Vision-Language Foundation Models</a></papertitle><br>
				Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, <strong>Zhengyuan Yang</strong>, Hanwang Zhang, Zicheng Liu, Lijuan Wang <br>
				ICCV 2023. <br>
				[<a href="https://arxiv.org/pdf/2303.14465.pdf">PDF</a>]
				[<a href="https://github.com/Wangt-CN/EqBen">Code</a>]
				[<a href="https://codalab.lisn.upsaclay.fr/competitions/10266">Benchmark</a>]
				[<a href="./bibtex/wang2023equivariant.txt">Bibtex</a>]
				<p><grey> EqBen explores the concept of equivariance in VLMs, focusing specifically on the multimodal similarity function that is not only the major training objective but also the core delivery to support downstream tasks. </grey></p>
			</td>
		</tr>
		
			<td width="25%">
				<a href="./LayoutBench.png">
				<img src="./LayoutBench.png" width="200px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2304.06671.pdf">Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation</a></papertitle><br>
				Jaemin Cho, Linjie Li, <strong>Zhengyuan Yang</strong>, Zhe Gan, Lijuan Wang, Mohit Bansal <br>
				Technical report <br>
				[<a href="https://arxiv.org/pdf/2304.06671.pdf">PDF</a>]
				[<a href="https://layoutbench.github.io/">Project Page</a>]
				[<a href="https://github.com/j-min/LayoutBench">Code</a>]
				[<a href="https://huggingface.co/spaces/j-min/IterInpaint-CLEVR">Demo</a>]
				[<a href="./bibtex/cho2023diagnostic.txt">Bibtex</a>]
				<p><grey> LayoutBench evaluates layout-guided image generation models with out-of-distribution (OOD) layouts in four skills: number, position, size, and shape. </grey></p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./reco/reco.png">
				<img src="./reco/reco.png" width="200px" height="100px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2211.15518.pdf">ReCo: Region-Controlled Text-to-Image Generation</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang <br>
				CVPR 2023. <br>
				[<a href="https://arxiv.org/pdf/2211.15518.pdf">PDF</a>]
				[<a href="http://github.com/microsoft/ReCo">Code</a>]
				[<a href="https://youtu.be/8X0zvl05mtU">Video</a>]
				[<a href="./bibtex/yang2022reco.txt">Bibtex</a>]
				<p><grey> ReCo is our new text-to-image model that allows the precise region control of input text queries. </grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./promptGPT.jpg">
				<img src="./promptGPT.jpg" width="200px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2210.09150.pdf">Prompting GPT-3 To Be Reliable</a></papertitle><br>
				Chenglei Si, Zhe Gan, <strong>Zhengyuan Yang</strong>, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, Lijuan Wang <br>
				ICLR 2023. <br>
				[<a href="https://arxiv.org/pdf/2210.09150.pdf">PDF</a>]
				[<a href="https://github.com/NoviScl/GPT3-Reliability">Code</a>]
				[<a href="https://twitter.com/ChengleiSi/status/1582473478543794177?s=20&t=AH7vbRJ9za77qJo9dVvSqg">Tweet</a>]
				[<a href="./bibtex/si2022prompting.txt">Bibtex</a>]
				<p><grey> Establish simple and effective prompts to demonstrate GPT-3's reliability in four facets: generalizability, fairness, calibration, and factuality. </grey></p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./unitab/unitab.jpg">
				<img src="./unitab/unitab.jpg" width="200px" height="100px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2111.12085.pdf">UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu, Lijuan Wang <br>
				ECCV 2022. <red>(Oral Presentation)</red> <br>
				[<a href="https://arxiv.org/pdf/2111.12085.pdf">PDF</a>]
				[<a href="https://github.com/microsoft/UniTAB">Code</a>]
				[<a href="./unitab/5226.pdf">Poster</a>]
				[<a href="./unitab/5226.mp4">Video</a>]
				[<a href="./bibtex/yang2022unitab.txt">Bibtex</a>]
				<p><grey> We propose UniTAB, a vision-language (VL) model that unifies text generation and bounding box prediction into a single architecture. </grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./reco/grit.png">
				<img src="./reco/grit.png" width="200px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2212.00280.pdf">GRiT: A Generative Region-to-text Transformer for Object Understanding</a></papertitle><br>
				Jialian Wu, Jianfeng Wang, <strong>Zhengyuan Yang</strong>, Zhe Gan, Zicheng Liu, Junsong Yuan, Lijuan Wang <br>
				Technical report <br>
				[<a href="https://arxiv.org/pdf/2212.00280.pdf">PDF</a>]
				[<a href="https://github.com/JialianW/GRiT">Code</a>]
				[<a href="./bibtex/wu2022grit.txt">Bibtex</a>]
				<p><grey> GRiT is a general object understanding framework that detects objects and describes them with any style of texts it was trained with, e.g., class names, object attributes, actions, counts, etc. </grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./transvg++.jpg">
				<img src="./transvg++.jpg" width="200px" height="100px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2206.06619.pdf">TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer</a></papertitle><br>
				Jiajun Deng, <strong>Zhengyuan Yang</strong>, Daqing Liu, Tianlang Chen, Wengang Zhou, Yanyong Zhang, Houqiang Li, Wanli Ouyang <br>
				TPAMI 2023. <br>
				[<a href="https://arxiv.org/pdf/2206.06619.pdf">PDF</a>]
				[<a href="https://github.com/djiajunustc/TransVG">Code</a>]
				[<a href="./bibtex/deng2022transvg++.txt">Bibtex</a>]
				<p><grey> Adapting Vision Transformer (ViT) for visual grounding. </grey></p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./fl-git/teaser.png">
				<img src="./fl-git/teaser.png" width="200px" height="100px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2205.14100.pdf">GIT: A Generative Image-to-text Transformer for Vision and Language</a></papertitle><br>
				Jianfeng Wang, <strong>Zhengyuan Yang</strong>, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang <br>
				TMLR 2022. <br>
				[<a href="https://arxiv.org/pdf/2205.14100.pdf">PDF</a>]
				[<a href="https://github.com/microsoft/GenerativeImage2Text">Code</a>]
				[<a href="./bibtex/wang2022git.txt">Bibtex</a>]
				<p><grey> Florence-GIT is our new multimodal generative foundation model. GIT shows a strong capbility of describing entities in the wild, such as scene texts, logos, landmarks, characters, etc. </grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./ccd.jpg">
				<img src="./ccd.jpg" width="200px" height="100px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2201.06734.pdf">Cross-modal Contrastive Distillation for Instructional Activity Anticipation</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Jingen Liu, Jing Huang, Xiaodong He, Tao Mei, Chenliang Xu, Jiebo Luo <br>
				ICPR 2022. <br>
				[<a href="https://arxiv.org/pdf/2201.06734.pdf">PDF</a>]
				[<a href="./ICPR_23.mp4">Video</a>]
				[<a href="./bibtex/yang2022cross.txt">Bibtex</a>]
				<p><grey> We propose cross-modal contrastive distillation (CCD) that facilities distilling teacher’s information to the student in a different modality.  </grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./ufo.jpg">
				<img src="./ufo.jpg" width="200px" height="100px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2111.10023.pdf">UFO: A UniFied TransfOrmer for Vision-Language Representation Learning</a></papertitle><br>
				Jianfeng Wang, Xiaowei Hu, Zhe Gan, <strong>Zhengyuan Yang</strong>, Xiyang Dai, Zicheng Liu, Yumao Lu, Lijuan Wang <br>
				Technical report <br>
				[<a href="https://arxiv.org/pdf/2111.10023.pdf">PDF</a>]
				[<a href="./bibtex/wang2021ufo.txt">Bibtex</a>]
				<p><grey> A single unified transformer(UFO), which is capable of processing either unimodal inputs or multimodal inputs, for VL representation learning. </grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./lemon.jpg">
				<img src="./lemon.jpg" height="140px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2111.12233.pdf">Scaling Up Vision-Language Pre-training for Image Captioning</a></papertitle><br>
				Xiaowei Hu, Zhe Gan, Jianfeng Wang, <strong>Zhengyuan Yang</strong>, Zicheng Liu, Yumao Lu, Lijuan Wang <br>
				CVPR 2022. <br>
				[<a href="https://arxiv.org/pdf/2111.12233.pdf">PDF</a>]
				[<a href="https://github.com/xiaoweihu/ALT200M">Code</a>]
				[<a href="./bibtex/deng2021transvg.txt">Bibtex</a>]
				<p><grey> The first empirical study on the scaling behavior of VLP for image captioning. </grey></p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./pica/intro.jpg">
				<img src="./pica/intro.jpg" width="160px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2109.05014.pdf">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, Lijuan Wang <br>
				AAAI 2022. <red>(Oral Presentation)</red> <br>
				[<a href="https://arxiv.org/pdf/2109.05014.pdf">PDF</a>]
				[<a href="https://github.com/microsoft/PICa">Code</a>]
				[<a href="./bibtex/yang2021empirical.txt">Bibtex</a>]
				[<a href="https://okvqa.allenai.org/leaderboard.html">Benchmarks</a>]
				<p><grey> Can GPT-3 benefit multimodal tasks? We provide an empirical study of GPT-3 for knowledge-based VQA, <a href="./pica/pica.jpg">named PICa.</a> </p><p>#1 in OKVQA leaderboard. (Sept. 2021)</p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./SAT/intro.jpg">
				<img src="./SAT/intro.jpg" width="200px" height="100px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2105.11450.pdf">SAT: 2D Semantics Assisted Training for 3D Visual Grounding</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Songyang Zhang, Liwei Wang, Jiebo Luo <br>
				ICCV 2021. <red>(Oral Presentation)</red> <br>
				[<a href="https://arxiv.org/pdf/2105.11450.pdf">PDF</a>]
				[<a href="https://github.com/zyang-ur/SAT">Code</a>]
				[<a href="./SAT/CVPR_REF3D_SAT.mp4">Video</a>]
				[<a href="./bibtex/yang2021sat.txt">Bibtex</a>]
				[<a href="https://referit3d.github.io/benchmarks.html">Benchmarks</a>]
				<p><grey> Boosting 3D visual grounding by using training-time 2D semantics.</grey></p><p>#1 in referit3d CVPR 2021 challenge.</p>
			</td>
		</tr>

			<td width="25%">
				<a href="./TransVG.jpg">
				<img src="./TransVG.jpg" width="150px" height="120px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2104.08541.pdf">TransVG: End-to-End Visual Grounding with Transformers</a></papertitle><br>
				Jiajun Deng, <strong>Zhengyuan Yang</strong>, Tianlang Chen, Wengang Zhou, Houqiang Li <br>
				ICCV 2021. <br>
				[<a href="https://arxiv.org/pdf/2104.08541.pdf">PDF</a>]
				[<a href="https://github.com/djiajunustc/TransVG">Code</a>]
				[<a href="./bibtex/deng2021transvg.txt">Bibtex</a>]
				<p><grey> A transformer-based framework for visual grounding. </grey></p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./TAP/intro.jpg">
				<img src="./TAP/intro.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2012.04638.pdf">TAP: Text-Aware Pre-training for Text-VQA and Text-Caption</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, Jiebo Luo <br>
				CVPR 2021. <red>(Oral Presentation)</red> <br>
				[<a href="https://arxiv.org/pdf/2012.04638.pdf">PDF</a>]
				[<a href="https://github.com/microsoft/TAP">Code</a>]
				[<a href="./TAP/CVPR2021_poster.pdf">Poster</a>]
				[<a href="./TAP/TAP_video.mp4">Video</a>]
				[<a href="./bibtex/yang2021tap.txt">Bibtex</a>]
				<p><grey> We propose Text-Aware Pre-training (TAP) for Text-VQA and Text-Caption tasks.</grey></p><p>#1 in TextCaps CVPR 2021 challenge.</p>
			</td>
		</tr>

			<td width="25%">
				<!-- <a href="./weak_grounding.png"> -->
				<img src="./weak_grounding.png" width="150px" height="120px">
<!-- 				<li>Click for zooming up.</li> -->
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2007.01951.pdf">Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation</a></papertitle><br>
				Liwei Wang, Jing Huang, Yin Li, Kun Xu, <strong>Zhengyuan Yang</strong>, Dong Yu <br>
				CVPR 2021. <br>
				[<a href="https://arxiv.org/pdf/2007.01951.pdf">PDF</a>]
				[<a href="https://github.com/jhuang81/weak-sup-visual-grounding">Code</a>]
				[<a href="./bibtex/wang2021improving.txt">Bibtex</a>]
				<p><grey> A weakly supervised visual grounding method that removes the need of object detection at test time.</grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./gti/gtiintro.jpg">
				<img src="./gti/gtiintro.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<!-- <papertitle>Grounding-Tracking-Integration</papertitle></a><br> -->
				<papertitle><a href="https://arxiv.org/pdf/1912.06316.pdf">Grounding-Tracking-Integration</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Tushar Kumar, Tianlang Chen, Jingsong Su, Jiebo Luo <br>
				IEEE T-CSVT. <br>
				[<a href="https://arxiv.org/pdf/1912.06316.pdf">PDF</a>]
				[<a href="./gti/LaSOT_updated.csv">Annotations</a>]
				[<a href="https://www.youtube.com/watch?v=Hex4_UElaS8">Demo1</a>]
				[<a href="https://www.youtube.com/watch?v=Ry3DBJzI-3M">Demo2</a>]
				[<a href="./bibtex/yang2020grounding.txt">Bibtex</a>]
				<p><grey> A simple yet effective modular framework for tracking by natural language specification.</grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./resq/resqintro.jpg">
				<img src="./resq/resqintro.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2008.01059.pdf">Improving One-stage Visual Grounding by Recursive Sub-query Construction</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Tianlang Chen, Liwei Wang, Jiebo Luo <br>
				ECCV 2020. <br>
				[<a href="https://arxiv.org/pdf/2008.01059.pdf">PDF</a>]
				[<a href="https://github.com/zyang-ur/ReSC">Code</a>]
				[<a href="./resq/2152_long.pdf">Slides</a>]
				[<a href="./resq/2152_long.mp4">Video</a>]
				[<a href="./bibtex/yang2020improving.txt">Bibtex</a>]
				<p><grey>  Improving one-stage visual grounding by addressing previous weaknesses in modeling long and complex queries.</grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./MM20-intro.jpg">
				<img src="./MM20-intro.jpg" width="200px" height="80px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2009.02016.pdf">Dynamic Context-guided Capsule Network for Multimodal Machine Translation</a></papertitle><br>
				Huan Lin, Fandong Meng, Jinsong Su, Yongjing Yin, <strong>Zhengyuan Yang</strong>, Yubin Ge, Jie Zhou, Jiebo Luo <br>
				ACMMM 2020. (Oral Presentation) <br>
				[<a href="https://arxiv.org/pdf/2009.02016.pdf">PDF</a>]
				[<a href="https://github.com/DeepLearnXMU/MM-DCCN">Code</a>]
				[<a href="./bibtex/lin2020dynamic.txt">Bibtex</a>]
				<p><grey> we propose a novel Dynamic Context-guided Capsule Network (DCCN) for multimodal machine translation.</grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./acl20.jpg">
				<img src="./acl20.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://www.aclweb.org/anthology/2020.acl-main.273.pdf">A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</a></papertitle><br>
				Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, <strong>Zhengyuan Yang</strong>, Jie Zhou, Jiebo Luo <br>
				ACL 2020. <br>
				[<a href="https://www.aclweb.org/anthology/2020.acl-main.273.pdf">PDF</a>]
				[<a href="./bibtex/yin2020novel.txt">Bibtex</a>]
				<p><grey> Multi-modal neural machine translation (NMT) with fine-grained cross-modality semantic correspondence.</grey></p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./ICCV19/VG_ICCV19.jpg">
				<img src="./ICCV19/VG_ICCV19.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="https://arxiv.org/pdf/1908.06354.pdf">A Fast and Accurate One-Stage Approach to Visual Grounding</a></papertitle></a><br>
				<strong>Zhengyuan Yang</strong>, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, Jiebo Luo <br>
				ICCV 2019. <red>(Oral Presentation)</red> <red>(187/4303=4.3%)</red> <br>
<!-- 				[<a href="./projects/conv_pose_att/TCSVT-01988-2018.pdf">PDF</a>]
				[<a href="https://drive.google.com/open?id=1_W6dbGu3ykDS3PYUkGGTKbz6soxwAxh6">UCF-Motion-Joints</a>] -->
				[<a href="https://arxiv.org/pdf/1908.06354.pdf">PDF</a>]
				[<a href="https://github.com/zyang-ur/onestage_grounding">Code</a>]
				[<a href="./ICCV19/ICCV19_slides.pdf">Slides</a>]
				[<a href="./ICCV19/ICCV19_poster.pdf">Poster</a>]
				[<a href="./bibtex/yang2019fast.txt">Bibtex</a>]
				<p><grey> A simple, fast, and accurate one-stage approach to visual grounding. 10 times faster and 7~20% higher in accuracy.</grey></p>
			</td>
		</tr>

		<!-- <tbody><tr bgcolor="#ffffd0"> -->
			<td width="25%">
				<a href="./pascal_visu.png">
				<img src="./pascal_visu.png" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="https://arxiv.org/pdf/1907.13051.pdf">Weakly Supervised Body Part Parsing with Pose based Part Priors</a></papertitle></a><br>
				<strong>Zhengyuan Yang</strong>, Yuncheng Li, Linjie Yang, Ning Zhang, Jiebo Luo <br>
				ICPR 2020. <br>
				[<a href="https://arxiv.org/pdf/1907.13051.pdf">PDF</a>]
				[<a href="http://cs.rochester.edu/u/zyang39/weakly_parsing/pascal_visu.html">Demo</a>]
				[<a href="./ICPR20/ICPR_poster_78.pdf">Poster</a>]
				[<a href="./ICPR20/ICPR_slides_78.pdf">Slides</a>]
				[<a href="./ICPR20/icpr_video_78.mp4">Video</a>]
				[<a href="./bibtex/yang2019weakly.txt">Bibtex</a>]
				<p><grey> Weakly-supervised body part parsing that achieves comparable results to the fully-supervised method with a same backbone.</grey></p>
			</td>
		</tr>

 			<td width="25%">
				<a href="./knn_body_language/CVPR18-fig-framework.jpg">
				<img src="./knn_body_language/CVPR18-fig-framework.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
				</a>			
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="./knn_body_language/ICPR20_Pose_Emotion_URMC.pdf">Pose-based Body Language Recognition for Emotion and Psychiatric Symptom Interpretation</a></papertitle></a><br>
				<!-- <papertitle>Pose-based Body Language Recognition for Emotion and Psychiatric Symptom Interpretation</papertitle></a><br> -->
				<strong>Zhengyuan Yang</strong>, Amanda Kay, Yuncheng Li, Wendi Cross, Jiebo Luo <br>
				ICPR 2020.<br>
				<!-- [PDF] -->
				[<a href="https://arxiv.org/pdf/2011.00043.pdf">PDF</a>]
				[<a href="./ICPR20/ICPR_poster_79.pdf">Poster</a>]
				[<a href="./ICPR20/ICPR_slides_79.pdf">Slides</a>]
				[<a href="./ICPR20/icpr_video_79.mp4">Video</a>]
				[<a href="./bibtex/yang2020pose.txt">Bibtex</a>]
				</p><p></p>
				<p><grey>A pose-based body language recognition framework for body language recognition and emotion interpretation.
				</grey></p>
			</td>
		</tr>

		<!-- <tbody><tr bgcolor="#ffffd0"> -->
			<td width="25%">
				<a href="./SG_cvpr19.jpg">
				<img src="./SG_cvpr19.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="https://arxiv.org/pdf/1811.10696.pdf">Attentive Relational Networks for Mapping Images to Scene Graphs</a></papertitle></a><br>
				Mengshi Qi, Weijian Li, <strong>Zhengyuan Yang</strong>, Yunhong Wang, Jiebo Luo<br>
				CVPR 2019. <br>
				[<a href="https://arxiv.org/pdf/1811.10696.pdf">PDF</a>]
				[<a href="./bibtex/qi2019attentive.txt">Bibtex</a>]
				<p><grey> A novel Attentive Relational Network for scene graph generation.</grey></p>
			</td>
		</tr>

		<!-- <tbody><tr bgcolor="#ffffd0"> -->
			<td width="25%">
				<a href="./projects/conv_pose_att/glan_all.jpg">
				<img src="./projects/conv_pose_att/w_module.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
				</a>			
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="./projects/conv_pose_att/TCSVT-01988-2018.pdf">Action Recognition with Spatio-Temporal Visual Attention on Skeleton Image Sequences</a></papertitle></a><br>
				<strong>Zhengyuan Yang</strong>, Yuncheng Li, Jianchao Yang, Jiebo Luo <br>
				ICPR 2018; IEEE T-CSVT <br>
				[<a href="./projects/conv_pose_att/TCSVT-01988-2018.pdf">PDF</a>]
				[<a href="https://drive.google.com/open?id=1_W6dbGu3ykDS3PYUkGGTKbz6soxwAxh6">UCF-Motion-Joints</a>]
				[<a href="./bibtex/yang2018action.txt">Bibtex</a>]
				<p><grey> A CNN-based approach for skeleton-based action recognition. SOTA on both clean 3D joints and noisy 2D estimated keypoints.</grey></p>
			</td>
		</tr>

		<!-- <tbody><tr bgcolor="#ffffd0"> -->
			<td width="25%">
				<a href="./emotion.png">
				<img src="./emotion.png" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="https://arxiv.org/pdf/1904.12201.pdf">Human-Centered Emotion Recognition in Animated GIFs with Facial Landmarks</a></papertitle></a><br>
				<strong>Zhengyuan Yang</strong>, Yixuan Zhang, Jiebo Luo <br>
				ICME 2019. <br>
				[<a href="https://arxiv.org/pdf/1904.12201.pdf">PDF</a>]
				[<a href="https://github.com/zyang-ur/human-centered-GIF">Data</a>]
				[<a href="./bibtex/yang2019human.txt">Bibtex</a>]
				<p><grey> Focusing on human faces to improve emotion recognition.</grey></p>
<!-- 				</p><p></p>
				<p>Motivated by the importance of human related information in GIFs, we propose a multi-modal multi-task framework for human-centered GIF emotion recognition.
				</p> -->
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<img src="./intern_saic/IMG_2205.jpg" width="200px" height="140px">
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="https://arxiv.org/abs/1801.06734.pdf">End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars with Visual Perception</a></papertitle></a><br>
				<strong>Zhengyuan Yang</strong>, Yixuan Zhang, Jerry Yu, Junjie Cai, Jiebo Luo <br>
				ICPR 2018. <a href="http://www.icpr2018.org/index.php?m=content&c=index&a=lists&catid=31%20">
				<em>Best Industry Related Paper Award (BIRPA)</em></a> <red>(1/1258=0.08%)</red> <br>
						<!-- <a href="./intern_saic/End-to-end_steering_control.pdf">Slides</a>
	      /-->
						[<a href="https://arxiv.org/abs/1801.06734.pdf">PDF</a>]
						[<a href="https://youtu.be/7QGI_tmwZhw">Demo</a>]
						[<a href="./bibtex/yang2018end.txt">Bibtex</a>]
				<p><grey> Building a prototype that controls the self-driving car's steering angle and speed. Check out the <a href="https://youtu.be/7QGI_tmwZhw">demo</a> that we recorded in the vehicle!</grey></p>
<!-- 				</p><p></p>
				<p>Taking an image or image sequences as input, predict steering angle and speed simultaneously end-to-end with neural networks.
				</p> -->
			</td>
		</tr>
			</tbody></table>
			

			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Internship</heading>
				</td>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./ms.jpg"  width="70" height="70">
				</td>
				
				<td width="75%" valign="center">
				<p>
					<a href="https://www.microsoft.com/en-us/research/">
					<papertitle>Microsoft, Redmond, WA </papertitle><br>
					</a>
					May - Aug 2020. Advisor: <a href="https://www.linkedin.com/in/yijuan-lu-590b426/"> Yijuan Lu, 
					<a href="http://jianfengwang.me/"> Jianfeng Wang,
					<a href="https://xiyinmsu.github.io/"> Xi Yin.
					</a><br>
					Project: Text-aware pre-training for Text-VQA and Text-Caption.
				</p>
				</td>
			</tr>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./tencent/tencent.jpg"  width="120" height="50">
				</td>
				
				<td width="75%" valign="center">
				<p>
					<a href="https://ai.tencent.com/ailab/en/index/">
					<papertitle>Tencent AI Lab, Bellevue, WA </papertitle><br>
					</a>
					Jan - Apr 2019. Advisor: <a href="http://boqinggong.info/"> Boqing Gong, 
					<a href="https://lwwangcse.github.io/"> Liwei Wang.
					</a><br>
					Project: Visual Grounding with Natural Language Quires.
				</p>
				</td>
			</tr>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./snap/snap.jpg"  width="70" height="70">
				</td>
				
				<td width="75%" valign="center">
				<p>
					<a href="https://www.snap.com/en-US/">
					<papertitle>SnapChat, Venice, CA </papertitle><br>
					</a>
					May - Aug 2018. Advisor: <a href="http://www.cs.rochester.edu/~yli/"> Yuncheng Li, 
					<a href="https://sites.google.com/site/linjieyang89/"> Linjie Yang, 
					<a href="https://scholar.google.com/citations?user=DplAah0AAAAJ&hl=en"> Ning Zhang.
					</a><br>
					Project: Weakly Supervised Human Part Parsing.
				</p>
				</td>
			</tr>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./intern_saic/SAIC.png"  width="70" height="70">
				</td>
				
				<td width="75%" valign="center">
				<p>
					<a href="http://www.saicusa.com/">
					<papertitle>SAIC Innovation Center, San Jose, CA </papertitle><br>
					</a>
					Jun - Aug 2017. Advisor: <a href="https://www.linkedin.com/in/jerry-yu/"> Jerry Yu. 
					</a>
					<br>
					Project: Steering Angle Control with End-to-end Neural Networks.
				</p>
				</td>
			</tr>



			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Awards</heading>
				</td>
			</tr>

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="http://www.sigmm.org/Awards/thesisaward">
					<awardtitle>2022 ACM SIGMM Award for Outstanding Ph.D. Thesis </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="https://eval.ai/web/challenges/challenge-page/906/overview">
					<awardtitle>Winner of CVPR 2021 TextCaps Challenge </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="https://referit3d.github.io/benchmarks.html">
					<awardtitle>Winner of CVPR 2021 ReferIt3D Challenge </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="https://blog.twitch.tv/en/2020/01/15/introducing-our-2020-twitch-research-fellows/">
					<awardtitle>Twitch Research Fellowship </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="./BIRPA.JPG">
					<awardtitle>Best Industry Related Paper Award at ICPR 2018 </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			
			</tbody></table>


			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Publications</heading>
				<ps>
					<li> Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, <strong>Zhengyuan Yang</strong>, Hanwang Zhang, Zicheng Liu, Lijuan Wang, "DisCo: Disentangled Control for Referring Human Dance Generation in Real World." 2023.
					[<a href="https://arxiv.org/pdf/2307.00040.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Linjie Li, Jianfeng Wang, Kevin Lin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action." 2023.
					[<a href="https://arxiv.org/pdf/2303.11381.pdf">PDF</a>]
					</li>
					<li> Yushi Hu, Hang Hua, <strong>Zhengyuan Yang</strong>, Weijia Shi, Noah A. Smith, Jiebo Luo, "PromptCap: Prompt-Guided Task-Aware Image Captioning," International Conference on Computer Vision (ICCV)</em>, Paris, France, Oct 2023.
					[<a href="https://arxiv.org/pdf/2211.09699.pdf">PDF</a>][<a href="https://github.com/Yushi-Hu/PromptCap">Code</a>]
					</li>
					<li> Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, <strong>Zhengyuan Yang</strong>, Hanwang Zhang, Zicheng Liu, Lijuan Wang, "Equivariant Similarity for Vision-Language Foundation Models," <em>International Conference on Computer Vision (ICCV)</em>, Paris, France, Oct 2023.
					[<a href="https://arxiv.org/pdf/2303.14465.pdf">PDF</a>][<a href="https://github.com/Wangt-CN/EqBen">Code</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, "ReCo: Region-Controlled Text-to-Image Generation," <em> IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, Vancouver, BC, June 2023.
					[<a href="https://arxiv.org/pdf/2211.15518.pdf">PDF</a>]
					</li>
					<li> Jaemin Cho, Linjie Li, <strong>Zhengyuan Yang</strong>, Zhe Gan, Lijuan Wang, Mohit Bansal, "Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation," 2023.
					[<a href="https://arxiv.org/pdf/2304.06671.pdf">PDF</a>][<a href="https://layoutbench.github.io/">Project page</a>]
					</li>			
					<li> Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, <strong>Zhengyuan Yang</strong>, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan Wang, Zicheng Liu, Houqiang Li, Nan Duan, "NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation," <em>Annual Meeting of the Association for Computational Linguistics (ACL)</em>, Toronto, Canada, July 2023. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2303.12346.pdf">PDF</a>]
					</li>
					<li> Xiaodong Wang, Chenfei Wu, Shengming Yin, Minheng Ni, Jianfeng Wang, Linjie Li, <strong>Zhengyuan Yang</strong>, Fan Yang, Lijuan Wang, Zicheng Liu, Yuejian Fang, Nan Duan, "Learning 3D Photography Videos via Self-supervised Diffusion on Single Images," <em>The 32nd International Joint Conference on Artificial Intelligence (IJCAI-23)</em>, Macao, August 2023.
					[<a href="https://arxiv.org/pdf/2302.10781.pdf">PDF</a>]
					</li>
					<li> Jialian Wu, Jianfeng Wang, <strong>Zhengyuan Yang</strong>, Zhe Gan, Zicheng Liu, Junsong Yuan, Lijuan Wang, "GRiT: A Generative Region-to-text Transformer for Object Understanding," 2022.
					[<a href="https://arxiv.org/pdf/2212.00280.pdf">PDF</a>][<a href="https://github.com/JialianW/GRiT">Code</a>]
					</li>
					<li> Chenglei Si, Zhe Gan, <strong>Zhengyuan Yang</strong>, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, Lijuan Wang, "Prompting GPT-3 To Be Reliable," <em>The Eleventh International Conference on Learning Representations (ICLR)</em>, Kigali, Rwanda, May 2023.
					[<a href="https://arxiv.org/pdf/2210.09150.pdf">PDF</a>][<a href="https://github.com/NoviScl/GPT3-Reliability">Code</a>]
					</li>
					<li> Jiajun Deng, <strong>Zhengyuan Yang</strong>, Daqing Liu, Tianlang Chen, Wengang Zhou, Yanyong Zhang, Houqiang Li, Wanli Ouyang, "TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer," IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2023.
					[<a href="https://arxiv.org/pdf/2206.06619.pdf">PDF</a>][<a href="https://github.com/djiajunustc/TransVG">Code</a>]
					</li>
					<li> Jianfeng Wang, <strong>Zhengyuan Yang</strong>, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang, "GIT: A Generative Image-to-text Transformer for Vision and Language," <em>Transactions on Machine Learning Research (TMLR)</em>, 2022.
					[<a href="https://arxiv.org/pdf/2205.14100.pdf">PDF</a>][<a href="https://github.com/microsoft/GenerativeImage2Text">Code</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu and Lijuan Wang, "UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling," <em>European Conference on Computer Vision (ECCV)</em>, Tel Aviv, Israel, October 2022. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2111.12085.pdf">PDF</a>][<a href="https://github.com/microsoft/UniTAB">Code</a>]
					</li>
					<li> Jianfeng Wang, Xiaowei Hu, Zhe Gan, <strong>Zhengyuan Yang</strong>, Xiyang Dai, Zicheng Liu, Yumao Lu and Lijuan Wang, "UFO: A UniFied TransfOrmer for Vision-Language Representation Learning," 2021.
					[<a href="https://arxiv.org/pdf/2111.10023.pdf">PDF</a>]
					</li>
					<li> Xiaowei Hu, Zhe Gan, Jianfeng Wang, <strong>Zhengyuan Yang</strong>, Zicheng Liu, Yumao Lu and Lijuan Wang, "Scaling Up Vision-Language Pre-training for Image Captioning," <em> IEEE Conference on Computer Vision and Pattern
					Recognition (CVPR)</em>, New Orleans, June 2022.
					[<a href="https://arxiv.org/pdf/2111.12233.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Jingen Liu, Jing Huang, Xiaodong He, Tao Mei, Chenliang Xu, Jiebo Luo, "Cross-modal Contrastive Distillation for Instructional Activity Anticipation," <em> International Conference on Pattern Recognition (ICPR)</em>, Montreal, Quebec, Canada, August 2022. (Oral Presentation).
					[<a href="https://arxiv.org/pdf/2201.06734.pdf">PDF</a>]
					</li>					
					<li> <strong>Zhengyuan Yang</strong>, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu and Lijuan Wang, "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA," <em> The 36th AAAI Conference on Artificial Intelligence (AAAI)</em>, February 2022. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2109.05014.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Songyang Zhang, Liwei Wang, Jiebo Luo, "SAT: 2D Semantics Assisted Training for 3D Visual Grounding," <em>International Conference on Computer Vision (ICCV)</em>, Oct 2021. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2105.11450.pdf">PDF</a>]
					</li>
					<li> Jiajun Deng, <strong>Zhengyuan Yang</strong>, Tianlang Chen, Wengang Zhou, Houqiang Li, "TransVG: End-to-End Visual Grounding with Transformers," <em>International Conference on Computer Vision (ICCV)</em>, Oct 2021.
					[<a href="https://arxiv.org/pdf/2104.08541.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, Jiebo Luo, "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption," <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2021. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2012.04638.pdf">PDF</a>]
					</li>
					<li> Liwei Wang, Jing Huang, Yin Li, Kun Xu, <strong>Zhengyuan Yang</strong>, Dong Yu, "Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation," <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2021.
					[<a href="https://arxiv.org/pdf/2007.01951.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Tianlang Chen, Liwei Wang, Jiebo Luo, "Improving One-stage Visual Grounding by Recursive Sub-query Construction," <em>European Conference on Computer Vision (ECCV)</em>, Glasgow, UK, August 2020.
					[<a href="https://arxiv.org/pdf/2008.01059.pdf">PDF</a>][<a href="https://github.com/zyang-ur/ReSC">Code</a>]
					</li>
					<li> Huan Lin, Fandong Meng, Jinsong Su, Yongjing Yin, <strong>Zhengyuan Yang</strong>, Yubin Ge, Jie Zhou, Jiebo Luo, "Dynamic Context-guided Capsule Network for Multimodal Machine Translation," <em>ACM Multimedia Conference (ACMMM)</em>, Seattle, WA, October 2020. (Oral Presentation)
					[<a href="https://arxiv.org/pdf/2009.02016.pdf">PDF</a>][<a href="https://github.com/DeepLearnXMU/MM-DCCN">Code</a>]
					</li>
					<li> Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, <strong>Zhengyuan Yang</strong>, Jie Zhou, Jiebo Luo, "A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation," <em>Annual Meeting of the Association for Computational Linguistics (ACL)</em>, Seattle, WA, July 2020.
					[<a href="https://www.aclweb.org/anthology/2020.acl-main.273.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Tushar Kumar, Tianlang Chen, Jingsong Su, Jiebo Luo, "Grounding-Tracking-Integration," IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT).
					[<a href="https://arxiv.org/pdf/1912.06316.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, Jiebo Luo, "A Fast and Accurate One-Stage Approach to Visual Grounding," <em>International Conference on Computer Vision (ICCV)</em>, Seoul, South Korea, October 2019. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/1908.06354.pdf">PDF</a>][<a href="https://github.com/zyang-ur/onestage_grounding">Code</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Yuncheng Li, Linjie Yang, Ning Zhang, Jiebo Luo, "Weakly Supervised Body Part Parsing with Pose based Part Priors," <em>International Conference on Pattern Recognition (ICPR)</em>, Millan, Italy, January, 2020.
					[<a href="https://arxiv.org/pdf/1907.13051.pdf">PDF</a>]
					[<a href="http://cs.rochester.edu/u/zyang39/weakly_parsing/pascal_visu.html">Demo</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Amanda Kay, Yuncheng Li, Wendi Cross, Jiebo Luo, "Pose-based Body Language Recognition for Emotion and Psychiatric Symptom Interpretation," <em>International Conference on Pattern Recognition (ICPR)</em>, Millan, Italy, January, 2020.
					[<a href="https://arxiv.org/pdf/2011.00043.pdf">PDF</a>]
					</li>
					<li> Mengshi Qi, Weijian Li, <strong>Zhengyuan Yang</strong>, Yunhong Wang, Jiebo Luo, "Attentive Relational Networks for Mapping Images to Scene Graphs," <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, Long Beach, USA, June 2019.
					[<a href="https://arxiv.org/pdf/1811.10696.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Yixuan Zhang, Jiebo Luo, "Human-Centered Emotion Recognition in Animated GIFs with Facial Landmarks," <em>International Conference on Multimedia and Expo (ICME)</em>, Shanghai, China, July 2019.
					[<a href="https://arxiv.org/pdf/1904.12201.pdf">PDF</a>]
					[<a href="https://github.com/zyang-ur/human-centered-GIF">Data</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Yuncheng Li, Jianchao Yang, Jiebo Luo, "Action Recognition with Spatio-Temporal Visual Attention on Skeleton Image Sequences," <em>IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT)</em>.
					[<a href="./projects/conv_pose_att/TCSVT-01988-2018.pdf">PDF</a>]
					[<a href="https://drive.google.com/open?id=1_W6dbGu3ykDS3PYUkGGTKbz6soxwAxh6">Data</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Yuncheng Li, Jianchao Yang, Jiebo Luo, "Action Recognition with Visual Attention on Skeleton Images," <em>International Conference on Pattern Recognition (ICPR)</em>, Beijing, China, August 2018. (Oral Presentation).
					[<a href="https://arxiv.org/pdf/1801.10304.pdf">PDF</a>]
					</li>	
					<li> <strong>Zhengyuan Yang</strong>, Yixuan Zhang, Jerry Yu, Junjie Cai, Jiebo Luo, "End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars with Visual Perceptions," <em>International Conference on Pattern Recognition (ICPR)</em>, Beijing, China, August 2018. (Oral Presentation)
					<a href="http://www.icpr2018.org/index.php?m=content&c=index&a=lists&catid=31%20"><em>Best Industry Related Paper Award (BIRPA)</em></a>.
					[<a href="https://arxiv.org/abs/1801.06734.pdf">PDF</a>]
					[<a href="https://youtu.be/7QGI_tmwZhw">Demo</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Wendi Cross, Jiebo Luo, "Personalized pose estimation for body language understanding," <em>International Conference on Image Processing (ICIP)</em>, Beijing, China, September 2017. (Oral Presentation)
					<!-- [<a href="./icip17/r4-personalized-pose.pdf">PDF</a>] -->
					</li>
				</ps>
				</td>
			</tr>
			</tbody></table>


			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Service</heading>
				</td>
			</tr>
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li><a href="https://eccv2022.ecva.net/program/outstanding-reviewers">
					<awardtitle>Outstanding Reviewer, ECCV 2022 </awardtitle></a>, <a href="http://cvpr2021.thecvf.com/node/184">
					<awardtitle>Outstanding Reviewer, CVPR 2021 </awardtitle><br>
					</a></li> 
				</p>
				</td>
			</tr>			
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle>Senior Program Committee (SPC): 37, 38th AAAI Conference on Artificial Intelligence (AAAI-23, 24)</awardtitle><br></li> 
				</p>
				</td>
			</tr>
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle>Associate Editor: IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</awardtitle><br></li> 
				</p>
				</td>
			</tr>
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle>Journal Reviewer: TPAMI, IJCV, TIP, TMM, TCybernetics, TCSVT, Pattern Recognition, Neurocomputing, TBioCAS, IEEE Access.</awardtitle><br></li> 
				</p>
				</td>
			</tr>
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle>Conference Reviewer: CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, ACL, EMNLP, AAAI, ACCV, WACV, ICME, ICIP.</awardtitle><br></li> 
				</p>
				</td>
			</tr>			
			</tbody></table>


			
<!-- 			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Teaching</heading>
				</td>
			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<a href="https://www.cs.rochester.edu/~gildea/2018_Spring/">
					<papertitle>TA CS246/446 - Spring 2018 </papertitle><br>
			Machine Learning
				</p>
				</td>
			</tr>
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<a href="https://www.cs.rochester.edu/courses/172/fall2017/">
					<papertitle>TA CS172 - Fall 2017 </papertitle><br>
			Datastructures and Algorithms
					</a>
				</p>
				</td>
			</tr>
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<a href="https://www.cs.rochester.edu/~ferguson/csc/242/Spring2017/syllabus.html">
					<papertitle>TA CS242 - Spring 2017 </papertitle><br>
			Intro to Artificial Intelligence
					</a>
				</p>
				</td>
			</tr>			
			</tbody></table>

 -->
			<center>
			<!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=1mhT__bUnjZx1JwGgr2z3jIRHGHFW7thIvYLz1NJMYY&cl=ffffff&w=a"></script> -->
			<!-- <a href="https://clustrmaps.com/site/1attr"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=1mhT__bUnjZx1JwGgr2z3jIRHGHFW7thIvYLz1NJMYY&cl=ffffff" width="100" /></a> -->
			<a href="https://clustrmaps.com/site/1bhzi"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=zcxXwQdctTsFPEw1SMscHvSL9xyUW3xNvytCTdI8loE&cl=ffffff" width="150" /></a>
			</center>

			<!-- <div style="display:inline-block;width:200px;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=0btq4qiwt1p&amp;m=1&amp;c=ff0000&amp;cr1=ffffff" async="async"></script></div> -->

			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="middle"><font size="2">
				&copy; 2023 Zhengyuan Yang. All rights reserved.
				<br>
				Template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">Jon Barron</a>. Thanks!
			<!--		  
			-->
					</font>
				</p>
				</td>
			</tr>
			</tbody></table>
<!-- 			<script type="text/javascript">
			var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
					document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
					
			</script><script src="./index_files/ga.js" type="text/javascript"></script> <script type="text/javascript">
			try {
					var pageTracker = _gat._getTracker("UA-7580334-1");
					pageTracker._trackPageview();
					} catch(err) {}
			</script> -->
		</td>
		</tr>
	</tbody></table>
	

</body></html>
