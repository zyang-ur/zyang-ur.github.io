<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0041)https://people.eecs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	<style type="text/css">
		/* Color scheme stolen from Sergey Karayev */
		a {
		color: #1772d0;
		text-decoration:none;
		}
		a:focus, a:hover {
		color: #f09228;
		text-decoration:none;
		}
		body,td,th,tr,p,a {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px
		}
		ps {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px
		}
		strong {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		red {
		color: #d02717;
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		grey {
		color: #696969;
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		heading {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 22px;
		}
		awardtitle {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 18px;
		/*font-weight: 700*/
		}
		papertitle {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		font-weight: 700
		}
		name {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 32px;
		}
		.one
		{
		width: 160px;
		height: 160px;
		position: relative;
		}
		.two
		{
		width: 160px;
		height: 160px;
		position: absolute;
		transition: opacity .2s ease-in-out;
		-moz-transition: opacity .2s ease-in-out;
		-webkit-transition: opacity .2s ease-in-out;
		}
		.fade {
		 transition: opacity .2s ease-in-out;
		 -moz-transition: opacity .2s ease-in-out;
		 -webkit-transition: opacity .2s ease-in-out;
		}
		span.highlight {
				background-color: #ffffd0;
		}
	</style>
	<!--<link rel="icon" type="image/png" href="https://people.eecs.berkeley.edu/~barron/seal_icon.png">-->
	<title>Zhengyuan Yang</title>
	
	<link href="./index_files/css" rel="stylesheet" type="text/css">
	</head>
	<body>
	<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
		<tbody><tr>
		<td>
			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td width="67%" valign="middle">
				<p align="center">
					<name>Zheng-yuan Yang</name>
				</p>
				<p>I am currently a Researcher at Microsoft. I received my Ph.D. degree in Computer Science at <a href="http://www.cs.rochester.edu/">University of Rochester</a>, Rochester, NY, advised by <a href="http://www.cs.rochester.edu/u/jluo/#VISTA">Prof. Jiebo Luo</a>. I did my bachelors at the <a href="http://en.sist.ustc.edu.cn/">University of Science and Technology of China</a>. I've received <a href="https://blog.twitch.tv/en/2020/01/15/introducing-our-2020-twitch-research-fellows/"> Twitch Research Fellowship</a> and the <a href="./BIRPA.JPG"> ICPR 2018 Best Industry Related Paper Award</a>. My research interests involve the intersection of computer vision and natural language processing.
				<br>
				<br>
				<br>
				<p align="center">
<a href="mailto:zhengyuan.yang13@gmail.com">Email</a> &nbsp;/&nbsp;
<a href="./resume_Zhengyuan_Yang.pdf">CV</a> &nbsp;/&nbsp;
<a href="https://github.com/zyang-ur">Github</a> &nbsp;/&nbsp;
<a href="https://scholar.google.com/citations?user=rP02ve8AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
<a href="https://www.linkedin.com/in/zhengyuan-yang-992b52105"> LinkedIn </a>
				</p>
				</td>
				<td width="37%">
				<!--<img src="./index_files/zyang2.jpg">-->
	<img src="./zyang2.jpg">
				</td>
			</tr>
			</tbody></table>


<!-- News -->

      <!-- <br> -->
      <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-news" id="news"><heading>News</heading></button>
      <!-- <div class="container"> -->
      <div id="content-news" class="collapse in">
      <!-- <div class="scroll">  -->

      <table border=0 class="bg_colour" style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
     
<!--           <tr>
              <td><p style="color:darkblue; display:inline">June '21 &nbsp</p></td>
              <td>We are the winner of <a href="https://eval.ai/web/challenges/challenge-page/906/overview">TextCaps Challenge 2021</a> and <a href="https://referit3d.github.io/benchmarks.html">ReferIt3D Challenge 2021</a>. Welcome to check the related <a href="https://arxiv.org/pdf/2012.04638.pdf">TAP</a> and <a href="https://arxiv.org/pdf/2105.11450.pdf">SAT</a> papers.</td>
          </tr>
          <tr>
              <td><p style="color:darkblue; display:inline">June '21 &nbsp</p></td>
              <td>I defensed my Ph.D. dissertation and will join Microsoft as a Researcher.</td>
          </tr>
          <tr>
              <td><p style="color:darkblue; display:inline">May '21 &nbsp</p></td>
              <td>I am selected as one of <a href="http://cvpr2021.thecvf.com/node/184">Outstanding Reviewer</a> for CVPR 2021.</td>
          </tr>
          <tr>
              <td><p style="color:darkblue; display:inline">Feb '21 &nbsp</p></td>
              <td>Two papers accepted by CVPR 2021 (The <a href="https://github.com/microsoft/TAP">TAP</a> paper was selected as Oral).</td>
          </tr>
 -->				
 				<ps>
					<li> [2021/09] &nbsp Can GPT-3 benefit multimodal tasks? We provide an empirical study of GPT-3 for knowledge-based VQA, <a href="https://arxiv.org/pdf/2109.05014.pdf">named PICa.</a> We show that prompting GPT-3 via the use of image captions with only 16 examples surpasses supervised sota by an absolute +8.6 points on the OK-VQA dataset (from 39.4 to 48.0).
					<li> [2021/07] &nbsp Two papers accepted by ICCV 2021 (The <a href="https://arxiv.org/pdf/2105.11450.pdf">SAT</a> paper was selected as Oral).
					<li> [2021/06] &nbsp We are the winner of <a href="https://eval.ai/web/challenges/challenge-page/906/overview">TextCaps Challenge 2021</a> and <a href="https://referit3d.github.io/benchmarks.html">ReferIt3D Challenge 2021</a>. Welcome to check the related <a href="https://arxiv.org/pdf/2012.04638.pdf">TAP</a> and <a href="https://arxiv.org/pdf/2105.11450.pdf">SAT</a> papers.
					</li>
					<li> [2021/06] &nbsp I defensed my Ph.D. dissertation <a href="https://www.proquest.com/docview/2572612109">"Visual Grounding: Building Cross-Modal Visual-Text Alignment"</a> and will join Microsoft as a Researcher.
					</li>
					<li> [2021/05] &nbsp I am selected as one of <a href="http://cvpr2021.thecvf.com/node/184">Outstanding Reviewer</a> for CVPR 2021.
					</li>
					<li> [2021/02] &nbsp Two papers accepted by CVPR 2021 (The <a href="https://github.com/microsoft/TAP">TAP</a> paper was selected as Oral).
					</li>
				</ps>


      </tbody></table>
      </div>
      <hr class="soft">
                


			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td width="100%" valign="middle">
					<heading>Research</heading>
					<p>
					My current research mainly focues on vision+language. I've also worked on human-centered visual understanding, such as human action recognition and parsing.
					Representative works are <span class="highlight">highlighted</span>.
					</p>
				</td>
			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./pica/intro.jpg">
				<img src="./pica/intro.jpg" width="160px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2109.05014.pdf">An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, Lijuan Wang <br>
				Technical report <br>
				[<a href="https://arxiv.org/pdf/2109.05014.pdf">PDF</a>]
				[Code]
				[<a href="./bibtex/yang2021empirical.txt">Bibtex</a>]
				[<a href="https://okvqa.allenai.org/leaderboard.html">Benchmarks</a>]
				<p><grey> Can GPT-3 benefit multimodal tasks? We provide an empirical study of GPT-3 for knowledge-based VQA, <a href="./pica/pica.jpg">named PICa.</a> </p><p>#1 in OKVQA leaderboard.</p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./SAT/intro.jpg">
				<img src="./SAT/intro.jpg" width="200px" height="100px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2105.11450.pdf">SAT: 2D Semantics Assisted Training for 3D Visual Grounding</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Songyang Zhang, Liwei Wang, Jiebo Luo <br>
				ICCV 2021. <red>(oral presentation)</red> <br>
				[<a href="https://arxiv.org/pdf/2105.11450.pdf">PDF</a>]
				[<a href="https://github.com/zyang-ur/SAT">Code</a>]
				[<a href="./bibtex/yang2021sat.txt">Bibtex</a>]
				[<a href="https://referit3d.github.io/benchmarks.html">Benchmarks</a>]
				<p><grey> Boosting 3D visual grounding by using training-time 2D semantics.</grey></p><p>#1 in referit3d CVPR 2021 challenge.</p>
			</td>
		</tr>

			<td width="25%">
				<a href="./TransVG.jpg">
				<img src="./TransVG.jpg" width="150px" height="120px">
 				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2104.08541.pdf">TransVG: End-to-End Visual Grounding with Transformers</a></papertitle><br>
				Jiajun Deng, <strong>Zhengyuan Yang</strong>, Tianlang Chen, Wengang Zhou, Houqiang Li <br>
				ICCV 2021. <br>
				[<a href="https://arxiv.org/pdf/2104.08541.pdf">PDF</a>]
				[<a href="https://github.com/djiajunustc/TransVG">Code</a>]
				[<a href="./bibtex/deng2021transvg.txt">Bibtex</a>]
				<p><grey> A transformer-based framework for visual grounding. </grey></p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./TAP/intro.jpg">
				<img src="./TAP/intro.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2012.04638.pdf">TAP: Text-Aware Pre-training for Text-VQA and Text-Caption</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, Jiebo Luo <br>
				CVPR 2021. <red>(oral presentation)</red> <br>
				[<a href="https://arxiv.org/pdf/2012.04638.pdf">PDF</a>]
				[<a href="https://github.com/microsoft/TAP">Code</a>]
				[<a href="./TAP/CVPR2021_poster.pdf">Poster</a>]
				[<a href="./TAP/TAP_video.mp4">Video</a>]
				[<a href="./bibtex/yang2021tap.txt">Bibtex</a>]
				<p><grey> We propose Text-Aware Pre-training (TAP) for Text-VQA and Text-Caption tasks.</grey></p><p>#1 in TextCaps CVPR 2021 challenge.</p>
			</td>
		</tr>

			<td width="25%">
				<!-- <a href="./weak_grounding.png"> -->
				<img src="./weak_grounding.png" width="150px" height="120px">
<!-- 				<li>Click for zooming up.</li> -->
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2007.01951.pdf">Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation</a></papertitle><br>
				Liwei Wang, Jing Huang, Yin Li, Kun Xu, <strong>Zhengyuan Yang</strong>, Dong Yu <br>
				CVPR 2021. <br>
				[<a href="https://arxiv.org/pdf/2007.01951.pdf">PDF</a>]
				[Code]
				[<a href="./bibtex/wang2021improving.txt">Bibtex</a>]
				<p><grey> A weakly supervised visual grounding method that removes the need of object detection at test time.</grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./gti/gtiintro.jpg">
				<img src="./gti/gtiintro.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<!-- <papertitle>Grounding-Tracking-Integration</papertitle></a><br> -->
				<papertitle><a href="https://arxiv.org/pdf/1912.06316.pdf">Grounding-Tracking-Integration</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Tushar Kumar, Tianlang Chen, Jingsong Su, Jiebo Luo <br>
				IEEE T-CSVT. <br>
				[<a href="https://arxiv.org/pdf/1912.06316.pdf">PDF</a>]
				[<a href="./gti/LaSOT_updated.csv">Annotations</a>]
				[<a href="https://www.youtube.com/watch?v=Hex4_UElaS8">Demo1</a>]
				[<a href="https://www.youtube.com/watch?v=Ry3DBJzI-3M">Demo2</a>]
				[<a href="./bibtex/yang2020grounding.txt">Bibtex</a>]
				<p><grey> A simple yet effective modular framework for tracking by natural language specification.</grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./resq/resqintro.jpg">
				<img src="./resq/resqintro.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2008.01059.pdf">Improving One-stage Visual Grounding by Recursive Sub-query Construction</a></papertitle><br>
				<strong>Zhengyuan Yang</strong>, Tianlang Chen, Liwei Wang, Jiebo Luo <br>
				ECCV 2020. <br>
				[<a href="https://arxiv.org/pdf/2008.01059.pdf">PDF</a>]
				[<a href="https://github.com/zyang-ur/ReSC">Code</a>]
				[<a href="./resq/2152_long.pdf">Slides</a>]
				[<a href="./resq/2152_long.mp4">Video</a>]
				[<a href="./bibtex/yang2020improving.txt">Bibtex</a>]
				<p><grey>  Improving one-stage visual grounding by addressing previous weaknesses in modeling long and complex queries.</grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./MM20-intro.jpg">
				<img src="./MM20-intro.jpg" width="200px" height="80px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://arxiv.org/pdf/2009.02016.pdf">Dynamic Context-guided Capsule Network for Multimodal Machine Translation</a></papertitle><br>
				Huan Lin, Fandong Meng, Jinsong Su, Yongjing Yin, <strong>Zhengyuan Yang</strong>, Yubin Ge, Jie Zhou, Jiebo Luo <br>
				ACMMM 2020. (oral presentation) <br>
				[<a href="https://arxiv.org/pdf/2009.02016.pdf">PDF</a>]
				[<a href="https://github.com/DeepLearnXMU/MM-DCCN">Code</a>]
				[<a href="./bibtex/lin2020dynamic.txt">Bibtex</a>]
				<p><grey> we propose a novel Dynamic Context-guided Capsule Network (DCCN) for multimodal machine translation.</grey></p>
			</td>
		</tr>

			<td width="25%">
				<a href="./acl20.jpg">
				<img src="./acl20.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="https://www.aclweb.org/anthology/2020.acl-main.273.pdf">A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation</a></papertitle><br>
				Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, <strong>Zhengyuan Yang</strong>, Jie Zhou, Jiebo Luo <br>
				ACL 2020. <br>
				[<a href="https://www.aclweb.org/anthology/2020.acl-main.273.pdf">PDF</a>]
				[<a href="./bibtex/yin2020novel.txt">Bibtex</a>]
				<p><grey> Multi-modal neural machine translation (NMT) with fine-grained cross-modality semantic correspondence.</grey></p>
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<a href="./ICCV19/VG_ICCV19.jpg">
				<img src="./ICCV19/VG_ICCV19.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="https://arxiv.org/pdf/1908.06354.pdf">A Fast and Accurate One-Stage Approach to Visual Grounding</a></papertitle></a><br>
				<strong>Zhengyuan Yang</strong>, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, Jiebo Luo <br>
				ICCV 2019. <red>(oral presentation)</red> <red>(187/4303=4.3%)</red> <br>
<!-- 				[<a href="./projects/conv_pose_att/TCSVT-01988-2018.pdf">PDF</a>]
				[<a href="https://drive.google.com/open?id=1_W6dbGu3ykDS3PYUkGGTKbz6soxwAxh6">UCF-Motion-Joints</a>] -->
				[<a href="https://arxiv.org/pdf/1908.06354.pdf">PDF</a>]
				[<a href="https://github.com/zyang-ur/onestage_grounding">Code</a>]
				[<a href="./ICCV19/ICCV19_slides.pdf">Slides</a>]
				[<a href="./ICCV19/ICCV19_poster.pdf">Poster</a>]
				[<a href="./bibtex/yang2019fast.txt">Bibtex</a>]
				<p><grey> A simple, fast, and accurate one-stage approach to visual grounding. 10 times faster and 7~20% higher in accuracy.</grey></p>
			</td>
		</tr>

		<!-- <tbody><tr bgcolor="#ffffd0"> -->
			<td width="25%">
				<a href="./pascal_visu.png">
				<img src="./pascal_visu.png" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="https://arxiv.org/pdf/1907.13051.pdf">Weakly Supervised Body Part Parsing with Pose based Part Priors</a></papertitle></a><br>
				<strong>Zhengyuan Yang</strong>, Yuncheng Li, Linjie Yang, Ning Zhang, Jiebo Luo <br>
				ICPR 2020. <br>
				[<a href="https://arxiv.org/pdf/1907.13051.pdf">PDF</a>]
				[<a href="http://cs.rochester.edu/u/zyang39/weakly_parsing/pascal_visu.html">Demo</a>]
				[<a href="./ICPR20/ICPR_poster_78.pdf">Poster</a>]
				[<a href="./ICPR20/ICPR_slides_78.pdf">Slides</a>]
				[<a href="./ICPR20/icpr_video_78.mp4">Video</a>]
				[<a href="./bibtex/yang2019weakly.txt">Bibtex</a>]
				<p><grey> Weakly-supervised body part parsing that achieves comparable results to the fully-supervised method with a same backbone.</grey></p>
			</td>
		</tr>

 			<td width="25%">
				<a href="./knn_body_language/CVPR18-fig-framework.jpg">
				<img src="./knn_body_language/CVPR18-fig-framework.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
				</a>			
			</td>
			<td valign="top" width="75%">
				<p>
				<papertitle><a href="./knn_body_language/ICPR20_Pose_Emotion_URMC.pdf">Pose-based Body Language Recognition for Emotion and Psychiatric Symptom Interpretation</a></papertitle></a><br>
				<!-- <papertitle>Pose-based Body Language Recognition for Emotion and Psychiatric Symptom Interpretation</papertitle></a><br> -->
				<strong>Zhengyuan Yang</strong>, Amanda Kay, Yuncheng Li, Wendi Cross, Jiebo Luo <br>
				ICPR 2020.<br>
				<!-- [PDF] -->
				[<a href="https://arxiv.org/pdf/2011.00043.pdf">PDF</a>]
				[<a href="./ICPR20/ICPR_poster_79.pdf">Poster</a>]
				[<a href="./ICPR20/ICPR_slides_79.pdf">Slides</a>]
				[<a href="./ICPR20/icpr_video_79.mp4">Video</a>]
				[<a href="./bibtex/yang2020pose.txt">Bibtex</a>]
				</p><p></p>
				<p><grey>A pose-based body language recognition framework for body language recognition and emotion interpretation.
				</grey></p>
			</td>
		</tr>

		<!-- <tbody><tr bgcolor="#ffffd0"> -->
			<td width="25%">
				<a href="./SG_cvpr19.jpg">
				<img src="./SG_cvpr19.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="https://arxiv.org/pdf/1811.10696.pdf">Attentive Relational Networks for Mapping Images to Scene Graphs</a></papertitle></a><br>
				Mengshi Qi, Weijian Li, <strong>Zhengyuan Yang</strong>, Yunhong Wang, Jiebo Luo<br>
				CVPR 2019. <br>
				[<a href="https://arxiv.org/pdf/1811.10696.pdf">PDF</a>]
				[<a href="./bibtex/qi2019attentive.txt">Bibtex</a>]
				<p><grey> A novel Attentive Relational Network for scene graph generation.</grey></p>
			</td>
		</tr>

		<!-- <tbody><tr bgcolor="#ffffd0"> -->
			<td width="25%">
				<a href="./projects/conv_pose_att/glan_all.jpg">
				<img src="./projects/conv_pose_att/w_module.jpg" width="200px" height="120px">
				<li>Click for zooming up.</li>
				</a>			
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="./projects/conv_pose_att/TCSVT-01988-2018.pdf">Action Recognition with Spatio-Temporal Visual Attention on Skeleton Image Sequences</a></papertitle></a><br>
				<strong>Zhengyuan Yang</strong>, Yuncheng Li, Jianchao Yang, Jiebo Luo <br>
				ICPR 2018; IEEE T-CSVT <br>
				[<a href="./projects/conv_pose_att/TCSVT-01988-2018.pdf">PDF</a>]
				[<a href="https://drive.google.com/open?id=1_W6dbGu3ykDS3PYUkGGTKbz6soxwAxh6">UCF-Motion-Joints</a>]
				[<a href="./bibtex/yang2018action.txt">Bibtex</a>]
				<p><grey> A CNN-based approach for skeleton-based action recognition. SOTA on both clean 3D joints and noisy 2D estimated keypoints.</grey></p>
			</td>
		</tr>

		<!-- <tbody><tr bgcolor="#ffffd0"> -->
			<td width="25%">
				<a href="./emotion.png">
				<img src="./emotion.png" width="200px" height="120px">
				<li>Click for zooming up.</li>
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="https://arxiv.org/pdf/1904.12201.pdf">Human-Centered Emotion Recognition in Animated GIFs with Facial Landmarks</a></papertitle></a><br>
				<strong>Zhengyuan Yang</strong>, Yixuan Zhang, Jiebo Luo <br>
				ICME 2019. <br>
				[<a href="https://arxiv.org/pdf/1904.12201.pdf">PDF</a>]
				[<a href="https://github.com/zyang-ur/human-centered-GIF">Data</a>]
				[<a href="./bibtex/yang2019human.txt">Bibtex</a>]
				<p><grey> Focusing on human faces to improve emotion recognition.</grey></p>
<!-- 				</p><p></p>
				<p>Motivated by the importance of human related information in GIFs, we propose a multi-modal multi-task framework for human-centered GIF emotion recognition.
				</p> -->
			</td>
		</tr>

		<tbody><tr bgcolor="#ffffd0">
			<td width="25%">
				<img src="./intern_saic/IMG_2205.jpg" width="200px" height="140px">
			</td>
			<td valign="top" width="75%">
				<p><!--<a href="https://arxiv.org/abs/1701.03077">-->
				<papertitle><a href="https://arxiv.org/abs/1801.06734.pdf">End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars with Visual Perception</a></papertitle></a><br>
				<strong>Zhengyuan Yang</strong>, Jerry Yu, Junjie Cai, Jiebo Luo <br>
				ICPR 2018. <a href="http://www.icpr2018.org/index.php?m=content&c=index&a=lists&catid=31%20">
				<em>Best Industry Related Paper Award (BIRPA)</em></a> <red>(1/1258=0.08%)</red> <br>
						<!-- <a href="./intern_saic/End-to-end_steering_control.pdf">Slides</a>
	      /-->
						[<a href="https://arxiv.org/abs/1801.06734.pdf">PDF</a>]
						[<a href="https://youtu.be/7QGI_tmwZhw">Demo</a>]
						[<a href="./bibtex/yang2018end.txt">Bibtex</a>]
				<p><grey> Building a prototype that controls the self-driving car's steering angle and speed. Check out the <a href="https://youtu.be/7QGI_tmwZhw">demo</a> that we recorded in the vehicle!</grey></p>
<!-- 				</p><p></p>
				<p>Taking an image or image sequences as input, predict steering angle and speed simultaneously end-to-end with neural networks.
				</p> -->
			</td>
		</tr>
			</tbody></table>
			

			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Internship</heading>
				</td>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./ms.jpg"  width="70" height="70">
				</td>
				
				<td width="75%" valign="center">
				<p>
					<a href="https://www.microsoft.com/en-us/research/">
					<papertitle>Microsoft, Redmond, WA </papertitle><br>
					</a>
					May - Aug 2020. Advisor: <a href="https://www.linkedin.com/in/yijuan-lu-590b426/"> Yijuan Lu, 
					<a href="https://sites.google.com/site/jianfengwanghome/"> Jianfeng Wang,
					<a href="https://xiyinmsu.github.io/"> Xi Yin.
					</a><br>
					Project: Text-aware pre-training for Text-VQA and Text-Caption.
				</p>
				</td>
			</tr>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./tencent/tencent.jpg"  width="120" height="50">
				</td>
				
				<td width="75%" valign="center">
				<p>
					<a href="https://ai.tencent.com/ailab/en/index/">
					<papertitle>Tencent AI Lab, Bellevue, WA </papertitle><br>
					</a>
					Jan - Apr 2019. Advisor: <a href="http://boqinggong.info/"> Boqing Gong, 
					<a href="https://lwwangcse.github.io/"> Liwei Wang.
					</a><br>
					Project: Visual Grounding with Natural Language Quires.
				</p>
				</td>
			</tr>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./snap/snap.jpg"  width="70" height="70">
				</td>
				
				<td width="75%" valign="center">
				<p>
					<a href="https://www.snap.com/en-US/">
					<papertitle>SnapChat, Venice, CA </papertitle><br>
					</a>
					May - Aug 2018. Advisor: <a href="http://www.cs.rochester.edu/~yli/"> Yuncheng Li, 
					<a href="https://sites.google.com/site/linjieyang89/"> Linjie Yang, 
					<a href="https://scholar.google.com/citations?user=DplAah0AAAAJ&hl=en"> Ning Zhang.
					</a><br>
					Project: Weakly Supervised Human Part Parsing.
				</p>
				</td>
			</tr>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./intern_saic/SAIC.png"  width="70" height="70">
				</td>
				
				<td width="75%" valign="center">
				<p>
					<a href="http://www.saicusa.com/">
					<papertitle>SAIC Innovation Center, San Jose, CA </papertitle><br>
					</a>
					Jun - Aug 2017. Advisor: <a href="https://www.linkedin.com/in/jerry-yu/"> Jerry Yu. 
					</a>
					<br>
					Project: Steering Angle Control with End-to-end Neural Networks.
				</p>
				</td>
			</tr>



			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Awards</heading>
				</td>
			</tr>

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="https://eval.ai/web/challenges/challenge-page/906/overview">
					<awardtitle>Winner of CVPR 2021 TextCaps Challenge </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="https://referit3d.github.io/benchmarks.html">
					<awardtitle>Winner of CVPR 2021 ReferIt3D Challenge </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="https://blog.twitch.tv/en/2020/01/15/introducing-our-2020-twitch-research-fellows/">
					<awardtitle>Twitch Research Fellowship </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="./BIRPA.JPG">
					<awardtitle>Best Industry Related Paper Award at ICPR 2018 </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			
			</tbody></table>


			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Publications</heading>
				<ps>
					<li> <strong>Zhengyuan Yang</strong>, Songyang Zhang, Liwei Wang, Jiebo Luo, "SAT: 2D Semantics Assisted Training for 3D Visual Grounding," <em>International Conference on Computer Vision (ICCV)</em>, Oct 2021. <red>(oral presentation)</red>
					[<a href="https://arxiv.org/pdf/2105.11450.pdf">PDF</a>]
					</li>
					<li> Jiajun Deng, <strong>Zhengyuan Yang</strong>, Tianlang Chen, Wengang Zhou, Houqiang Li, "TransVG: End-to-End Visual Grounding with Transformers," <em>International Conference on Computer Vision (ICCV)</em>, Oct 2021.
					[<a href="https://arxiv.org/pdf/2104.08541.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, Jiebo Luo, "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption," <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2021. <red>(oral presentation)</red>
					[<a href="https://arxiv.org/pdf/2012.04638.pdf">PDF</a>]
					</li>
					<li> Liwei Wang, Jing Huang, Yin Li, Kun Xu, <strong>Zhengyuan Yang</strong>, Dong Yu, "Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation," <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, June 2021.
					[<a href="https://arxiv.org/pdf/2007.01951.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Tianlang Chen, Liwei Wang, Jiebo Luo, "Improving One-stage Visual Grounding by Recursive Sub-query Construction," <em>European Conference on Computer Vision (ECCV)</em>, Glasgow, UK, August 2020.
					[<a href="https://arxiv.org/pdf/2008.01059.pdf">PDF</a>][<a href="https://github.com/zyang-ur/ReSC">Code</a>]
					</li>
					<li> Huan Lin, Fandong Meng, Jinsong Su, Yongjing Yin, <strong>Zhengyuan Yang</strong>, Yubin Ge, Jie Zhou, Jiebo Luo, "Dynamic Context-guided Capsule Network for Multimodal Machine Translation," <em>ACM Multimedia Conference (ACMMM)</em>, Seattle, WA, October 2020. (oral presentation)
					[<a href="https://arxiv.org/pdf/2009.02016.pdf">PDF</a>][<a href="https://github.com/DeepLearnXMU/MM-DCCN">Code</a>]
					</li>
					<li> Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, <strong>Zhengyuan Yang</strong>, Jie Zhou, Jiebo Luo, "A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation," <em>Annual Meeting of the Association for Computational Linguistics (ACL)</em>, Seattle, WA, July 2020.
					[<a href="https://www.aclweb.org/anthology/2020.acl-main.273.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Tushar Kumar, Tianlang Chen, Jingsong Su, Jiebo Luo, "Grounding-Tracking-Integration," IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT).
					[<a href="https://arxiv.org/pdf/1912.06316.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, Jiebo Luo, "A Fast and Accurate One-Stage Approach to Visual Grounding," <em>International Conference on Computer Vision (ICCV)</em>, Seoul, South Korea, October 2019. <red>(oral presentation)</red>
					[<a href="https://arxiv.org/pdf/1908.06354.pdf">PDF</a>][<a href="https://github.com/zyang-ur/onestage_grounding">Code</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Yuncheng Li, Linjie Yang, Ning Zhang, Jiebo Luo, "Weakly Supervised Body Part Parsing with Pose based Part Priors," <em>International Conference on Pattern Recognition (ICPR)</em>, Millan, Italy, January, 2020.
					[<a href="https://arxiv.org/pdf/1907.13051.pdf">PDF</a>]
					[<a href="http://cs.rochester.edu/u/zyang39/weakly_parsing/pascal_visu.html">Demo</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Amanda Kay, Yuncheng Li, Wendi Cross, Jiebo Luo, "Pose-based Body Language Recognition for Emotion and Psychiatric Symptom Interpretation," <em>International Conference on Pattern Recognition (ICPR)</em>, Millan, Italy, January, 2020.
					[<a href="https://arxiv.org/pdf/2011.00043.pdf">PDF</a>]
					</li>
					<li> Mengshi Qi, Weijian Li, <strong>Zhengyuan Yang</strong>, Yunhong Wang, Jiebo Luo, "Attentive Relational Networks for Mapping Images to Scene Graphs," <em>Conference on Computer Vision and Pattern Recognition (CVPR)</em>, Long Beach, USA, June 2019.
					[<a href="https://arxiv.org/pdf/1811.10696.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Yixuan Zhang, Jiebo Luo, "Human-Centered Emotion Recognition in Animated GIFs with Facial Landmarks," <em>International Conference on Multimedia and Expo (ICME)</em>, Shanghai, China, July 2019.
					[<a href="https://arxiv.org/pdf/1904.12201.pdf">PDF</a>]
					[<a href="https://github.com/zyang-ur/human-centered-GIF">Data</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Yuncheng Li, Jianchao Yang, Jiebo Luo, "Action Recognition with Spatio-Temporal Visual Attention on Skeleton Image Sequences," <em>IEEE Transactions on Circuits and Systems for Video Technology (T-CSVT)</em>.
					[<a href="./projects/conv_pose_att/TCSVT-01988-2018.pdf">PDF</a>]
					[<a href="https://drive.google.com/open?id=1_W6dbGu3ykDS3PYUkGGTKbz6soxwAxh6">Data</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Yuncheng Li, Jianchao Yang, Jiebo Luo, "Action Recognition with Visual Attention on Skeleton Images," <em>International Conference on Pattern Recognition (ICPR)</em>, Beijing, China, August 2018. (oral presentation).
					[<a href="https://arxiv.org/pdf/1801.10304.pdf">PDF</a>]
					</li>	
					<li> <strong>Zhengyuan Yang</strong>, Yixuan Zhang, Jerry Yu, Junjie Cai, Jiebo Luo, "End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars with Visual Perceptions," <em>International Conference on Pattern Recognition (ICPR)</em>, Beijing, China, August 2018. (oral presentation)
					<a href="http://www.icpr2018.org/index.php?m=content&c=index&a=lists&catid=31%20"><em>Best Industry Related Paper Award (BIRPA)</em></a>.
					[<a href="https://arxiv.org/abs/1801.06734.pdf">PDF</a>]
					[<a href="https://youtu.be/7QGI_tmwZhw">Demo</a>]
					</li>
					<li> <strong>Zhengyuan Yang</strong>, Wendi Cross, Jiebo Luo, "Personalized pose estimation for body language understanding," <em>International Conference on Image Processing (ICIP)</em>, Beijing, China, September 2017. (oral presentation)
					<!-- [<a href="./icip17/r4-personalized-pose.pdf">PDF</a>] -->
					</li>
				</ps>
				</td>
			</tr>
			</tbody></table>


			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Service</heading>
				</td>
			</tr>
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li><a href="http://cvpr2021.thecvf.com/node/184">
					<awardtitle>Outstanding Reviewer, CVPR 2021 </awardtitle><br>
					</a></li> 
				</p>
				</td>
			</tr>			
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle>Journal Reviewer: TIP, TMM, TCybernetics, TCSVT, Pattern Recognition, Neurocomputing, TBioCAS, IEEE Access.</awardtitle><br></li> 
				</p>
				</td>
			</tr>
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle>Conference Reviewer: CVPR, ICCV, NeurIPS, ICLR, ICML, ACL, EMNLP, AAAI, ACCV, WACV, ICME, ICIP.</awardtitle><br></li> 
				</p>
				</td>
			</tr>			
			</tbody></table>


			
			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Teaching</heading>
				</td>
			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<a href="https://www.cs.rochester.edu/~gildea/2018_Spring/">
					<papertitle>TA CS246/446 - Spring 2018 </papertitle><br>
			Machine Learning
				</p>
				</td>
			</tr>
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<a href="https://www.cs.rochester.edu/courses/172/fall2017/">
					<papertitle>TA CS172 - Fall 2017 </papertitle><br>
			Datastructures and Algorithms
					</a>
				</p>
				</td>
			</tr>
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<a href="https://www.cs.rochester.edu/~ferguson/csc/242/Spring2017/syllabus.html">
					<papertitle>TA CS242 - Spring 2017 </papertitle><br>
			Intro to Artificial Intelligence
					</a>
				</p>
				</td>
			</tr>			
			</tbody></table>


			<center>
			<!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=1mhT__bUnjZx1JwGgr2z3jIRHGHFW7thIvYLz1NJMYY&cl=ffffff&w=a"></script> -->
			<!-- <a href="https://clustrmaps.com/site/1attr"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=1mhT__bUnjZx1JwGgr2z3jIRHGHFW7thIvYLz1NJMYY&cl=ffffff" width="100" /></a> -->
			<a href="https://clustrmaps.com/site/1bhzi"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=zcxXwQdctTsFPEw1SMscHvSL9xyUW3xNvytCTdI8loE&cl=ffffff" width="100" /></a>
			</center>

			<!-- <div style="display:inline-block;width:200px;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=0btq4qiwt1p&amp;m=1&amp;c=ff0000&amp;cr1=ffffff" async="async"></script></div> -->

			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="middle"><font size="2">
				&copy; 2021 Zhengyuan Yang. All rights reserved.
				<br>
				Template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">Jon Barron</a>. Thanks!
			<!--		  
			-->
					</font>
				</p>
				</td>
			</tr>
			</tbody></table>
<!-- 			<script type="text/javascript">
			var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
					document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
					
			</script><script src="./index_files/ga.js" type="text/javascript"></script> <script type="text/javascript">
			try {
					var pageTracker = _gat._getTracker("UA-7580334-1");
					pageTracker._trackPageview();
					} catch(err) {}
			</script> -->
		</td>
		</tr>
	</tbody></table>
	

</body></html>
