<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- saved from url=(0041)https://people.eecs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
	<style type="text/css">
		/* Color scheme stolen from Sergey Karayev */
		a {
		color: #1772d0;
		text-decoration:none;
		}
		a:focus, a:hover {
		color: #f09228;
		text-decoration:none;
		}
		body,td,th,tr,p,a {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px
		}
		ps {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px
		}
		strong {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		red {
		color: #ff0000;
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		font-weight: bold;
		}
/*		red {
		color: #d02717;
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		}*/
		grey {
		color: #696969;
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		heading {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 22px;
		}
		px14 {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		}
		awardtitle {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 18px;
		/*font-weight: 700*/
		}
		yeartitle {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 16px;
		/*font-weight: 700*/
		}
		papertitle {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 14px;
		font-weight: 700
		}
		name {
		font-family: 'Lato', Verdana, Helvetica, sans-serif;
		font-size: 32px;
		}
		li {
		  margin: 2px;
		}
    .gap {
      margin-top: 0px;
      margin-bottom: 7px;
    }
		.one
		{
		width: 160px;
		height: 160px;
		position: relative;
		}
		.two
		{
		width: 160px;
		height: 160px;
		position: absolute;
		transition: opacity .2s ease-in-out;
		-moz-transition: opacity .2s ease-in-out;
		-webkit-transition: opacity .2s ease-in-out;
		}
		.fade {
		 transition: opacity .2s ease-in-out;
		 -moz-transition: opacity .2s ease-in-out;
		 -webkit-transition: opacity .2s ease-in-out;
		}
		span.highlight {
				background-color: #ffffd0;
		}
	</style>
	<link rel="icon" type="image/png" href="./icon.png">
	<title>Zhengyuan Yang</title>
	
	<link href="./index_files/css" rel="stylesheet" type="text/css">
	</head>
	<body>
	<table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
		<tbody><tr>
		<td>
			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td width="67%" valign="middle">
				<p align="center">
					<name>Zhengyuan Yang</name>
				</p>
				<p>I am currently a <a href="https://www.microsoft.com/en-us/research/project/azure-florence-vision-and-language/">Senior Researcher at Microsoft</a>. I received my Ph.D. degree in Computer Science at <a href="http://www.cs.rochester.edu/">University of Rochester</a>, advised by <a href="http://www.cs.rochester.edu/u/jluo/#VISTA">Prof. Jiebo Luo</a>. I did my bachelors at <a href="http://en.sist.ustc.edu.cn/">University of Science and Technology of China</a>. I've received <a href="http://www.sigmm.org/Awards/thesisaward">ACM SIGMM Award for Outstanding Ph.D. Thesis</a>, <a href="https://blog.twitch.tv/en/2020/01/15/introducing-our-2020-twitch-research-fellows/"> Twitch Research Fellowship</a>, and <a href="./BIRPA.JPG"> ICPR 2018 Best Industry Related Paper Award</a>. My research interests involve the intersection of computer vision and natural language processing, including multi-modal vision-language understanding and generation.
				<br>
				<br>
				<br>
				<p align="center">
<a href="mailto:zhengyuan.yang13@gmail.com">Email</a> &nbsp;/&nbsp;
<a href="./resume_Zhengyuan_Yang.pdf">CV</a> &nbsp;/&nbsp;
<a href="https://github.com/zyang-ur">Github</a> &nbsp;/&nbsp;
<a href="https://scholar.google.com/citations?user=rP02ve8AAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp;
<a href="https://www.linkedin.com/in/zhengyuan-yang-992b52105"> LinkedIn </a> &nbsp;/&nbsp;
<a href="https://translate.google.com/?sl=zh-CN&tl=en&text=%E6%9D%A8%20%E5%BE%81%E5%85%83&op=translate"> Name Pronounce </a> &nbsp;/&nbsp;
<a href="#section_pub"> Publications </a>
				</p>
				</td>
				<td width="37%">
				<!--<img src="./index_files/zyang2.jpg">-->
<!-- 	<img src="./zyang2.jpg"> -->
	<!-- <img src="./zyang2.jpg"> -->
<img src='./zyang2.jpg' onmouseover="this.src='./IMG_3086.jpg';" onmouseout="this.src='./zyang2.jpg';" />
				</td>
			</tr>
			</tbody></table>


<!-- News -->

      <!-- <br> -->
      <button style="border:0px transparent; background-color: transparent;outline:none;"type="button" class="collapsible" data-toggle="collapse" data-target="#content-news" id="news"><heading>News</heading></button>
      <!-- <div class="container"> -->
      <div id="content-news" class="collapse in">
      <!-- <div class="scroll">  -->

      <table border=0 class="bg_colour" style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      	<br>
     
<!--           <tr>
              <td><p style="color:darkblue; display:inline">June '21 &nbsp</p></td>
              <td>We are the winner of <a href="https://eval.ai/web/challenges/challenge-page/906/overview">TextCaps Challenge 2021</a> and <a href="https://referit3d.github.io/benchmarks.html">ReferIt3D Challenge 2021</a>. Welcome to check the related <a href="https://arxiv.org/pdf/2012.04638.pdf">TAP</a> and <a href="https://arxiv.org/pdf/2105.11450.pdf">SAT</a> papers.</td>
          </tr>
          <tr>
              <td><p style="color:darkblue; display:inline">June '21 &nbsp</p></td>
              <td>I defensed my Ph.D. dissertation and will join Microsoft as a Researcher.</td>
          </tr>
          <tr>
              <td><p style="color:darkblue; display:inline">May '21 &nbsp</p></td>
              <td>I am selected as one of <a href="http://cvpr2021.thecvf.com/node/184">Outstanding Reviewer</a> for CVPR 2021.</td>
          </tr>
          <tr>
              <td><p style="color:darkblue; display:inline">Feb '21 &nbsp</p></td>
              <td>Two papers accepted to CVPR 2021 (The <a href="https://github.com/microsoft/TAP">TAP</a> paper was selected as Oral).</td>
          </tr>
 -->
 				<ps>
					<li class="gap"> <px14>[2024/02]</px14> &nbsp Four papers accepted to CVPR 2024: (1) <a href="https://mm-narrator.github.io/">MM-Narrator</a>, audio descriptions (AD) generation with GPT-4, (2) <a href="https://disco-dance.github.io/">DisCo</a>, human dance generation with disentangled controls, (3) Tuning diffusion models towards diverse image generation, (4) <a href="https://mmsum-dataset.github.io/">MMSum</a>, a dataset for video multimodal summarization. </li>
					<li class="gap"> <px14>[2023/12]</px14> &nbsp I will serve as an Area Chair for ACMMM 2024, and an Exhibits and Demos Chair for ICME 2024. <a href="https://2024.ieeeicme.org/demonstrations/">Welcome to submit your demo papers!</a></li>
 					<li class="gap"> <px14>[2023/11]</px14> &nbsp How would it be if LMMs could interact with smartphones as humans do? Checkout <a href="https://github.com/zzxslp/MM-Navigator">GPT-4V in Wonderland: Large Multimodal Models for Zero-Shot Smartphone GUI Navigation</a>. [<a href="https://aimodels.substack.com/p/researchers-taught-gpt-4v-to-use">Article</a>]</li>
 					<li class="gap"> <px14>[2023/11]</px14> &nbsp How could LMMs contribute to social good? Checkout <a href="https://github.com/VIStA-H/GPT-4V_Social_Media#gpt-4vision-as-a-social-media-analysis-engine">GPT-4V(ision) as A Social Media Analysis Engine</a>.</li>
 					<li class="gap"> <px14>[2023/10]</px14> &nbsp How might LMMs revolutionize the understanding of video and streaming content? Checkout <a href="https://multimodal-vid.github.io/">MM-Vid: Advancing Video Understanding with GPT-4V(ision)</a>.</li>
 					<li class="gap"> <px14>[2023/10]</px14> &nbsp How well can image generation models assist visual design? Checkout <a href="https://design-bench.github.io/">DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual Design</a>.</li>
 					<li class="gap"> <px14>[2023/10]</px14> &nbsp How can LMM-based agents achieve human-like multimodal iterative exploration? Checkout our initial study on <a href="https://idea2img.github.io/">a generative agent, named Idea2Img<img src="./icon.png" alt="Idea2Img" width="16"/>,</a> focusing on automatic image design and generation. <a href="https://www.youtube.com/watch?v=TL2A8MYXsCE"> Thanks for the great video!</a> </li>
 					<li class="gap"> <px14>[2023/09]</px14> &nbsp What are the current state and promising future directions for large multimodal models (LMMs)? Please checkout our Preliminary Explorations with GPT-4V(ision): <a href="https://arxiv.org/abs/2309.17421">The Dawn of LMMs: Preliminary Explorations with GPT-4V(ision)</a>. </li>
					<li class="gap"> <px14>[2023/09]</px14> &nbsp Please checkout our survey paper/book on <a href="https://arxiv.org/abs/2309.10020">Multimodal Foundation Models: From Specialists to General-Purpose Assistants</a>. [<a href="https://vlp-tutorial.github.io/">Slides</a>] [<a href="https://www.youtube.com/playlist?list=PLB1k029in3UhWaAsXP1DGq8qEpWxW0QyS">YouTube</a>] [<a href="https://space.bilibili.com/20119746/channel/collectiondetail?sid=1480989">Bilibili</a>] </li>
					<li> <px14>[2023/08]</px14> &nbsp <a href="https://github.com/yuweihao/MM-Vet">MM-Vet</a> is an LMM evaluation benchmark that evaluates Large Multimodal Models' integrated VL capabilities. [<a href="https://paperswithcode.com/sota/visual-question-answering-on-mm-vet?tag_filter=0">MM-Vet Leaderbaord</a>]</li>
					<li> <px14>[2023/07]</px14> &nbsp Two papers accepted to ICCV 2023: (1) <a href="https://yushi-hu.github.io/promptcap_demo/">PromptCap</a>, prompt controlled visual captioning; (2) <a href="https://github.com/Wangt-CN/EqBen">EQBen</a>, a new diagnostic VLM benchmark.
					<li> <px14>[2023/06]</px14> &nbsp I will serve as a SPC member for AAAI 2024.
					<li> <px14>[2023/06]</px14> &nbsp Check out our CVPR 2023 Tutorial on <a href="https://vlp-tutorial.github.io/">"Recent Advances in Vision Foundation Models"</a>. Slides and recordings availble.
					<li> <px14>[2023/03]</px14> &nbsp We build <a href="https://multimodal-react.github.io">MM-REACT</a>, a system paradigm that integrates LLMs with a pool of vision experts to achieve multimodal reasoning and action.
					<li> <px14>[2023/03]</px14> &nbsp <em>IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</em> special issue on <a href="https://ieee-cas.org/files/ieeecass/2023-03/Call%20for%20papers%20%28AIGC%29_final_0306.pdf">"AI-Generated Content for Multimedia."</a> Submission deadline: July 1st, 2023.
					<li> <px14>[2023/02]</px14> &nbsp <a href="https://arxiv.org/pdf/2211.15518.pdf">ReCo</a> is our new text-to-image model that allows the precise region control of input text queries, accepted to CVPR 2023. <a href="./reco/reco.png">See a teaser here.</a>
					<li> <px14>[2023/01]</px14> &nbsp <a href="https://arxiv.org/pdf/2210.09150.pdf">Prompting GPT-3 To Be Reliable</a> accepted to ICLR 2023.
					<li> <px14>[2022/10]</px14> &nbsp My Ph.D. thesis <a href="https://www.proquest.com/docview/2572612109">"Visual Grounding: Building Cross-Modal Visual-Text Alignment"</a> wins the <a href="http://www.sigmm.org/Awards/thesisaward">2022 ACM SIGMM Award for Outstanding Ph.D. Thesis</a>.
					<li> <px14>[2022/10]</px14> &nbsp I am selected as one of <a href="https://eccv2022.ecva.net/program/outstanding-reviewers">Outstanding Reviewers</a> for ECCV 2022.
					</li>
					<li> <px14>[2022/07]</px14> &nbsp <a href="https://arxiv.org/pdf/2111.12085.pdf">UniTAB</a> accepted to ECCV 2022 as an Oral presentation.
					<li> <px14>[2022/07]</px14> &nbsp I will serve as a SPC member for AAAI 2023.
					<li> <px14>[2022/06]</px14> &nbsp Check out our CVPR 2022 Tutorial on <a href="https://vlp-tutorial.github.io/">"Recent Advances in Vision-and-Language Pre-training"</a>. Slides and recordings availble.
					<li> <px14>[2022/05]</px14> &nbsp The new multimodal generative foundation model <a href="https://arxiv.org/pdf/2205.14100.pdf">Florence-GIT</a> achieves new sota across 12 image/video VL tasks, including the first human-parity on TextCaps. GIT achieves 88.79% ImageNet-1k accuracy using a generative scheme. <a href="./fl-git/teaser.png">See a teaser here.</a>
					<li> <px14>[2022/01]</px14> &nbsp I will serve as an Associate Editor for <a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=76">IEEE TCSVT</a>.
<!-- 					<li> <px14>[2021/09]</px14> &nbsp Can GPT-3 benefit multimodal tasks? We provide an empirical study of GPT-3 for knowledge-based VQA, <a href="https://arxiv.org/pdf/2109.05014.pdf">named PICa.</a> (Selected as Oral in AAAI 2022)
					<li> [2021/07] &nbsp Two papers accepted to ICCV 2021 (The <a href="https://arxiv.org/pdf/2105.11450.pdf">SAT</a> paper was selected as Oral).
					<li> [2021/06] &nbsp We are the winner of <a href="https://eval.ai/web/challenges/challenge-page/906/overview">TextCaps Challenge 2021</a> and <a href="https://referit3d.github.io/benchmarks.html">ReferIt3D Challenge 2021</a>. Welcome to check the related <a href="https://arxiv.org/pdf/2012.04638.pdf">TAP</a> and <a href="https://arxiv.org/pdf/2105.11450.pdf">SAT</a> papers.
					</li>
					<li> [2021/06] &nbsp I defensed my Ph.D. dissertation <a href="https://www.proquest.com/docview/2572612109">"Visual Grounding: Building Cross-Modal Visual-Text Alignment"</a> and will join <a href="https://www.microsoft.com/en-us/research/project/azure-florence-vision-and-language/">Microsoft as a Researcher</a>.
					</li>
					<li> [2021/05] &nbsp I am selected as one of <a href="http://cvpr2021.thecvf.com/node/184">Outstanding Reviewers</a> for CVPR 2021.
					</li>
					<li> [2021/02] &nbsp Two papers accepted to CVPR 2021 (The <a href="https://github.com/microsoft/TAP">TAP</a> paper was selected as Oral). -->
					</li>
				</ps>


      </tbody></table>
      </div>
      <hr class="soft">
                

      <table border=0 class="bg_colour" style="padding:10px;width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


			<!-- <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"> -->
			<tbody><tr>
				<td>
				<heading id="section_pub">Selected Publications</heading>
				  <br>
					<p>
					My current research mainly focues on multimodal generation and understanding. Please check the <a href="https://scholar.google.com/citations?user=rP02ve8AAAAJ&hl=en">Google Scholar</a> for more complete and up-to-date publication list.
					</p>

 				<br><yeartitle><b>arXiv preprints</b></yeartitle><br><br>
				<ps>
					<li class="gap"> An Yan, <strong>Zhengyuan Yang</strong>, Junda Wu, Wanrong Zhu, Jianwei Yang, Linjie Li, Kevin Lin, Jianfeng Wang, Julian McAuley, Jianfeng Gao, Lijuan Wang, "List Items One by One: A New Data Source and Learning Paradigm for Multimodal LLMs"
					[<a href="https://arxiv.org/pdf/2404.16375.pdf">PDF</a>][<a href="https://github.com/zzxslp/SoM-LLaVA">Code</a>]
					</li>
					<li class="gap"> Chenglei Si*, Yanzhe Zhang*, <strong>Zhengyuan Yang</strong>, Ruibo Liu, Diyi Yang, "Design2Code: How Far Are We From Automating Front-End Engineering?"
					[<a href="https://arxiv.org/pdf/2403.03163.pdf">PDF</a>][<a href="https://github.com/NoviScl/Design2Code">Code</a>][<a href="https://salt-nlp.github.io/Design2Code/">Project page</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang*</strong>, Linjie Li*, Kevin Lin*, Jianfeng Wang*, Chung-Ching Lin*, Zicheng Liu, Lijuan Wang, "The dawn of lmms: Preliminary explorations with gpt-4v (ision)."
					[<a href="https://arxiv.org/pdf/2309.17421.pdf">PDF</a>][<a href="https://openai.com/contributions/gpt-4v">Acknowledgments</a>] <red>(Exploratory work cataloguing use of GPT-4V)</red> 
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Jianfeng Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, "Idea2Img: Iterative Self-Refinement with GPT-4V(ision) for Automatic Image Design and Generation."
					[<a href="https://arxiv.org/pdf/2310.08541.pdf">PDF</a>][<a href="https://github.com/zyang-ur/idea2img">Code</a>][<a href="https://idea2img.github.io/">Project page</a>][<a href="https://www.youtube.com/watch?v=TL2A8MYXsCE">Video</a>]
					</li>
					<li class="gap"> An Yan*, <strong>Zhengyuan Yang*</strong>, Wanrong Zhu, Kevin Lin, Linjie Li, Jianfeng Wang, Jianwei Yang, Yiwu Zhong, Julian McAuley, Jianfeng Gao, Zicheng Liu, Lijuan Wang, "Gpt-4v in wonderland: Large multimodal models for zero-shot smartphone gui navigation."
					[<a href="https://arxiv.org/pdf/2311.07562.pdf">PDF</a>][<a href="https://github.com/zzxslp/MM-Navigator">Code</a>]
					</li>
					<li class="gap"> Kevin Lin*, Faisal Ahmed*, Linjie Li*, Chung-Ching Lin*, Ehsan Azarnasab, <strong>Zhengyuan Yang</strong>, Jianfeng Wang, Lin Liang, Zicheng Liu, Yumao Lu, Ce Liu, Lijuan Wang, "Mm-vid: Advancing video understanding with gpt-4v (ision)."
					[<a href="https://arxiv.org/pdf/2310.19773.pdf">PDF</a>][<a href="https://multimodal-vid.github.io/">Project page</a>]
					</li>
					<li class="gap"> Weihao Yu*, <strong>Zhengyuan Yang*</strong>, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, Lijuan Wang, "MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities."
					[<a href="https://arxiv.org/pdf/2308.02490.pdf">PDF</a>][<a href="https://github.com/yuweihao/MM-Vet/">Code</a>][<a href="https://paperswithcode.com/sota/visual-question-answering-on-mm-vet?tag_filter=0">Leaderbaord</a>]
					</li>
					<li> Hanjia Lyu*, Jinfa Huang*, Daoan Zhang*, Yongsheng Yu*, Xinyi Mou, Jinsheng Pan, <strong>Zhengyuan Yang</strong>, Zhongyu Wei, Jiebo Luo, "Gpt-4v (ision) as a social media analysis engine."
					[<a href="https://arxiv.org/pdf/2311.07547.pdf">PDF</a>][<a href="https://github.com/VIStA-H/GPT-4V_Social_Media">Code</a>]
					</li>
					<li> Zecheng Tang, Chenfei Wu, Zekai Zhang, Mingheng Ni, Shengming Yin, Yu Liu, <strong>Zhengyuan Yang</strong>, Lijuan Wang, Zicheng Liu, Juntao Li, Nan Duan, "StrokeNUWA: Tokenizing Strokes for Vector Graphic Synthesis."
					[<a href="https://arxiv.org/pdf/2401.17093.pdf">PDF</a>]
					</li>	
					<li> Alex Jinpeng Wang, Linjie Li, Kevin Qinghong Lin, Jianfeng Wang, Kevin Lin, <strong>Zhengyuan Yang</strong>, Lijuan Wang, Mike Zheng Shou, "COSMO: COntrastive Streamlined MultimOdal Model with Interleaved Pre-Training."
					[<a href="https://arxiv.org/pdf/2401.00849.pdf">PDF</a>][<a href="https://fingerrec.github.io/cosmo/">Project page</a>]
					</li>	
					<li> Xueyan Zou, Linjie Li, Jianfeng Wang, Jianwei Yang, Mingyu Ding, <strong>Zhengyuan Yang</strong>, Feng Li, Hao Zhang, Shilong Liu, Arul Aravinthan, Yong Jae Lee, Lijuan Wang, "Interfacing Foundation Models' Embeddings."
					[<a href="https://arxiv.org/pdf/2312.07532.pdf">PDF</a>][<a href="https://github.com/UX-Decoder/FIND">Code</a>]
					</li>	
					<li> Jie An, <strong>Zhengyuan Yang</strong>, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Lijuan Wang, Jiebo Luo, "Openleaf: Open-domain interleaved image-text generation and evaluation."
					[<a href="https://arxiv.org/pdf/2310.07749.pdf">PDF</a>]
					</li>	
					<li> Kevin Lin*, <strong>Zhengyuan Yang*</strong>, Linjie Li, Jianfeng Wang, Lijuan Wang, "DEsignBench: Exploring and Benchmarking DALL-E 3 for Imagining Visual Design."
					[<a href="https://arxiv.org/pdf/2310.15144.pdf">PDF</a>][<a href="https://design-bench.github.io/">Project page</a>]
					</li>			
					<li> Jialian Wu, Jianfeng Wang, <strong>Zhengyuan Yang</strong>, Zhe Gan, Zicheng Liu, Junsong Yuan, Lijuan Wang, "GRiT: A Generative Region-to-text Transformer for Object Understanding."
					[<a href="https://arxiv.org/pdf/2212.00280.pdf">PDF</a>][<a href="https://github.com/JialianW/GRiT">Code</a>]
					</li>
				</ps>

 				<br><yeartitle><b>2024</b></yeartitle><br><br>
				<ps>
					<li> Jie An, <strong>Zhengyuan Yang</strong>, Jianfeng Wang, Linjie Li, Zicheng Liu, Lijuan Wang, Jiebo Luo, "Bring Metric Functions into Diffusion Models," <em>The 33rd International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>)</em>, Jeju, August 2024.
					[<a href="https://arxiv.org/pdf/2401.02414.pdf">PDF</a>]
					</li>	
					<li> Jaemin Cho, Linjie Li, <strong>Zhengyuan Yang</strong>, Zhe Gan, Lijuan Wang, Mohit Bansal, "Diagnostic Benchmark and Iterative Inpainting for Layout-Guided Image Generation," <em>CVPR Workshop on the Evaluation of Generative Foundation Models</em>, Seattle, WA, June 2024.
					[<a href="https://arxiv.org/pdf/2304.06671.pdf">PDF</a>][<a href="https://layoutbench.github.io/">Project page</a>]
					</li>	
					<li class="gap"> Chaoyi Zhang, Kevin Lin, <strong>Zhengyuan Yang</strong>, Jianfeng Wang, Linjie Li, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, "MM-Narrator: Narrating Long-form Videos with Multimodal In-Context Learning," <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, Seattle, WA, June 2024. <red>(Highlight Presentation)</red>
					[<a href="https://arxiv.org/pdf/2311.17435.pdf">PDF</a>][<a href="https://mm-narrator.github.io/">Project page</a>]
					</li>
					<li class="gap"> Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin, <strong>Zhengyuan Yang</strong>, Hanwang Zhang, Zicheng Liu, Lijuan Wang, "DisCo: Disentangled Control for Referring Human Dance Generation in Real World," <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, Seattle, WA, June 2024.
					[<a href="https://arxiv.org/pdf/2307.00040.pdf">PDF</a>][<a href="https://github.com/Wangt-CN/DisCo">Code</a>][<a href="https://disco-dance.github.io/">Project page</a>]
					</li>
					<li class="gap"> Zichen Miao, Jiang Wang, Ze Wang, <strong>Zhengyuan Yang</strong>, Lijuan Wang, Qiang Qiu, Zicheng Liu, "Training Diffusion Models Towards Diverse Image Generation with Reinforcement Learning," <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, Seattle, WA, June 2024.
					</li>
					<li class="gap"> Jielin Qiu, Jiacheng Zhu, William Han, Aditesh Kumar, Karthik Mittal, Claire Jin, <strong>Zhengyuan Yang</strong>, Linjie Li, Jianfeng Wang, Ding Zhao, Bo Li, Lijuan Wang, "MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos," <em>IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, Seattle, WA, June 2024. <red>(Highlight Presentation)</red>
					[<a href="https://arxiv.org/pdf/2306.04216.pdf">PDF</a>][<a href="https://mmsum-dataset.github.io/">Project page</a>]
					</li>
				</ps>

 				<br><yeartitle><b>2023</b></yeartitle><br><br>
				<ps>
					<li class="gap"> Chunyuan Li*, Zhe Gan*, <strong>Zhengyuan Yang*</strong>, Jianwei Yang*, Linjie Li*, Lijuan Wang, Jianfeng Gao, "Multimodal Foundation Models: From Specialists to General-Purpose Assistants," Foundations and Trends in Computer Graphics and Vision, 2023. <red>(A survey book on multimodal foundation models)</red> 
					[<a href="https://arxiv.org/pdf/2309.10020.pdf">PDF</a>]
					</li>
					<li> <strong>Zhengyuan Yang*</strong>, Linjie Li*, Jianfeng Wang*, Kevin Lin*, Ehsan Azarnasab*, Faisal Ahmed*, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, "MM-REACT: Prompting ChatGPT for Multimodal Reasoning and Action."
					[<a href="https://arxiv.org/pdf/2303.11381.pdf">PDF</a>][<a href="https://github.com/microsoft/MM-REACT">Code</a>][<a href="https://multimodal-react.github.io/">Project page</a>]
					</li>
					<li class="gap"> Yushi Hu, Hang Hua, <strong>Zhengyuan Yang</strong>, Weijia Shi, Noah A. Smith, Jiebo Luo, "PromptCap: Prompt-Guided Task-Aware Image Captioning," <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, Paris, France, Oct 2023.
					[<a href="https://arxiv.org/pdf/2211.09699.pdf">PDF</a>][<a href="https://github.com/Yushi-Hu/PromptCap">Code</a>]
					</li>
					<li class="gap"> Tan Wang, Kevin Lin, Linjie Li, Chung-Ching Lin, <strong>Zhengyuan Yang</strong>, Hanwang Zhang, Zicheng Liu, Lijuan Wang, "Equivariant Similarity for Vision-Language Foundation Models," <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, Paris, France, Oct 2023. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2303.14465.pdf">PDF</a>][<a href="https://github.com/Wangt-CN/EqBen">Code</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, Lijuan Wang, "ReCo: Region-Controlled Text-to-Image Generation," <em> IEEE Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, Vancouver, BC, June 2023.
					[<a href="https://arxiv.org/pdf/2211.15518.pdf">PDF</a>][<a href="https://github.com/microsoft/ReCo">Code</a>]
					</li>
					<li class="gap"> Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni, <strong>Zhengyuan Yang</strong>, Linjie Li, Shuguang Liu, Fan Yang, Jianlong Fu, Gong Ming, Lijuan Wang, Zicheng Liu, Houqiang Li, Nan Duan, "NUWA-XL: Diffusion over Diffusion for eXtremely Long Video Generation," <em>Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>)</em>, Toronto, Canada, July 2023. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2303.12346.pdf">PDF</a>][<a href="https://msra-nuwa.azurewebsites.net/#/NUWAXL">Project page</a>]
					</li>
					<li class="gap"> Xiaodong Wang, Chenfei Wu, Shengming Yin, Minheng Ni, Jianfeng Wang, Linjie Li, <strong>Zhengyuan Yang</strong>, Fan Yang, Lijuan Wang, Zicheng Liu, Yuejian Fang, Nan Duan, "Learning 3D Photography Videos via Self-supervised Diffusion on Single Images," <em>The 32nd International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>)</em>, Macao, August 2023.
					[<a href="https://arxiv.org/pdf/2302.10781.pdf">PDF</a>]
					</li>
					<li class="gap"> Chenglei Si, Zhe Gan, <strong>Zhengyuan Yang</strong>, Shuohang Wang, Jianfeng Wang, Jordan Boyd-Graber, Lijuan Wang, "Prompting GPT-3 To Be Reliable," <em>The Eleventh International Conference on Learning Representations (<strong>ICLR</strong>)</em>, Kigali, Rwanda, May 2023.
					[<a href="https://arxiv.org/pdf/2210.09150.pdf">PDF</a>][<a href="https://github.com/NoviScl/GPT3-Reliability">Code</a>]
					</li>
					<li class="gap"> Jiajun Deng, <strong>Zhengyuan Yang</strong>, Daqing Liu, Tianlang Chen, Wengang Zhou, Yanyong Zhang, Houqiang Li, Wanli Ouyang, "TransVG++: End-to-End Visual Grounding with Language Conditioned Vision Transformer," IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>), 2023.
					[<a href="https://arxiv.org/pdf/2206.06619.pdf">PDF</a>][<a href="https://github.com/djiajunustc/TransVG">Code</a>]
					</li>
				</ps>
 				<br><yeartitle><b>2022</b></yeartitle><br><br>
				<ps>
					<li class="gap"> Jianfeng Wang, <strong>Zhengyuan Yang</strong>, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, Lijuan Wang, "GIT: A Generative Image-to-text Transformer for Vision and Language," <em>Transactions on Machine Learning Research (<strong>TMLR</strong>)</em>, 2022.
					[<a href="https://arxiv.org/pdf/2205.14100.pdf">PDF</a>][<a href="https://github.com/microsoft/GenerativeImage2Text">Code</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Faisal Ahmed, Zicheng Liu, Yumao Lu and Lijuan Wang, "UniTAB: Unifying Text and Box Outputs for Grounded Vision-Language Modeling," <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, Tel Aviv, Israel, October 2022. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2111.12085.pdf">PDF</a>][<a href="https://github.com/microsoft/UniTAB">Code</a>]
					</li>
					<li class="gap"> Jianfeng Wang, Xiaowei Hu, Zhe Gan, <strong>Zhengyuan Yang</strong>, Xiyang Dai, Zicheng Liu, Yumao Lu and Lijuan Wang, "UFO: A UniFied TransfOrmer for Vision-Language Representation Learning."
					[<a href="https://arxiv.org/pdf/2111.10023.pdf">PDF</a>]
					</li>
					<li class="gap"> Xiaowei Hu, Zhe Gan, Jianfeng Wang, <strong>Zhengyuan Yang</strong>, Zicheng Liu, Yumao Lu and Lijuan Wang, "Scaling Up Vision-Language Pre-training for Image Captioning," <em> IEEE Conference on Computer Vision and Pattern
					Recognition (<strong>CVPR</strong>)</em>, New Orleans, June 2022.
					[<a href="https://arxiv.org/pdf/2111.12233.pdf">PDF</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Jingen Liu, Jing Huang, Xiaodong He, Tao Mei, Chenliang Xu, Jiebo Luo, "Cross-modal Contrastive Distillation for Instructional Activity Anticipation," <em> International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, Montreal, Quebec, Canada, August 2022. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2201.06734.pdf">PDF</a>]
					</li>					
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu and Lijuan Wang, "An Empirical Study of GPT-3 for Few-Shot Knowledge-Based VQA," <em> The 36th AAAI Conference on Artificial Intelligence (<strong>AAAI</strong>)</em>, February 2022. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2109.05014.pdf">PDF</a>][<a href="https://github.com/microsoft/pica">Code</a>]
					</li>
				</ps>
 				<br><yeartitle><b>PhD Thesis</b></yeartitle><br><br>
				<ps>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, "Visual Grounding: Building Cross-Modal Visual-Text Alignment," <em>University of Rochester</em>. <red>(ACM SIGMM Award for Outstanding Ph.D. Thesis)</red> [<a href="https://www.proquest.com/docview/2572612109">PDF</a>]
					</li>
				</ps>
 				<br><yeartitle><b>2021</b></yeartitle><br><br>
				<ps>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Songyang Zhang, Liwei Wang, Jiebo Luo, "SAT: 2D Semantics Assisted Training for 3D Visual Grounding," <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, Oct 2021. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2105.11450.pdf">PDF</a>][<a href="https://github.com/zyang-ur/sat">Code</a>]
					</li>
					<li class="gap"> Jiajun Deng, <strong>Zhengyuan Yang</strong>, Tianlang Chen, Wengang Zhou, Houqiang Li, "TransVG: End-to-End Visual Grounding with Transformers," <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, Oct 2021.
					[<a href="https://arxiv.org/pdf/2104.08541.pdf">PDF</a>][<a href="https://github.com/djiajunustc/TransVG">Code</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Florencio, Lijuan Wang, Cha Zhang, Lei Zhang, Jiebo Luo, "TAP: Text-Aware Pre-training for Text-VQA and Text-Caption," <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, June 2021. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2012.04638.pdf">PDF</a>][<a href="https://github.com/microsoft/tap">Code</a>]
					</li>
					<li class="gap"> Liwei Wang, Jing Huang, Yin Li, Kun Xu, <strong>Zhengyuan Yang</strong>, Dong Yu, "Improving Weakly Supervised Visual Grounding by Contrastive Knowledge Distillation," <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, June 2021.
					[<a href="https://arxiv.org/pdf/2007.01951.pdf">PDF</a>][<a href="https://github.com/jhuang81/weak-sup-visual-grounding">Code</a>]
					</li>
				</ps>
 				<br><yeartitle><b>2020</b></yeartitle><br><br>
				<ps>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Tianlang Chen, Liwei Wang, Jiebo Luo, "Improving One-stage Visual Grounding by Recursive Sub-query Construction," <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, Glasgow, UK, August 2020.
					[<a href="https://arxiv.org/pdf/2008.01059.pdf">PDF</a>][<a href="https://github.com/zyang-ur/ReSC">Code</a>]
					</li>
					<li class="gap"> Huan Lin, Fandong Meng, Jinsong Su, Yongjing Yin, <strong>Zhengyuan Yang</strong>, Yubin Ge, Jie Zhou, Jiebo Luo, "Dynamic Context-guided Capsule Network for Multimodal Machine Translation," <em>ACM Multimedia Conference (<strong>ACMMM</strong>)</em>, Seattle, WA, October 2020. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/2009.02016.pdf">PDF</a>][<a href="https://github.com/DeepLearnXMU/MM-DCCN">Code</a>]
					</li>
					<li class="gap"> Yongjing Yin, Fandong Meng, Jinsong Su, Chulun Zhou, <strong>Zhengyuan Yang</strong>, Jie Zhou, Jiebo Luo, "A Novel Graph-based Multi-modal Fusion Encoder for Neural Machine Translation," <em>Annual Meeting of the Association for Computational Linguistics (<strong>ACL</strong>)</em>, Seattle, WA, July 2020.
					[<a href="https://www.aclweb.org/anthology/2020.acl-main.273.pdf">PDF</a>][<a href="https://github.com/ARIES-LM/GMNMT">Code</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Tushar Kumar, Tianlang Chen, Jingsong Su, Jiebo Luo, "Grounding-Tracking-Integration," IEEE Transactions on Circuits and Systems for Video Technology (<strong>T-CSVT</strong>).
					[<a href="https://arxiv.org/pdf/1912.06316.pdf">PDF</a>]
					</li>
				</ps>
 				<br><yeartitle><b>2019 and Earlier</b></yeartitle><br><br>
				<ps>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Boqing Gong, Liwei Wang, Wenbing Huang, Dong Yu, Jiebo Luo, "A Fast and Accurate One-Stage Approach to Visual Grounding," <em>International Conference on Computer Vision (<strong>ICCV</strong></em>, Seoul, South Korea, October 2019. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/1908.06354.pdf">PDF</a>][<a href="https://github.com/zyang-ur/onestage_grounding">Code</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Yuncheng Li, Linjie Yang, Ning Zhang, Jiebo Luo, "Weakly Supervised Body Part Parsing with Pose based Part Priors," <em>International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, Millan, Italy, January, 2020.
					[<a href="https://arxiv.org/pdf/1907.13051.pdf">PDF</a>]
					[<a href="http://cs.rochester.edu/u/zyang39/weakly_parsing/pascal_visu.html">Demo</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Amanda Kay, Yuncheng Li, Wendi Cross, Jiebo Luo, "Pose-based Body Language Recognition for Emotion and Psychiatric Symptom Interpretation," <em>International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, Millan, Italy, January, 2020.
					[<a href="https://arxiv.org/pdf/2011.00043.pdf">PDF</a>]
					</li>
					<li class="gap"> Mengshi Qi, Weijian Li, <strong>Zhengyuan Yang</strong>, Yunhong Wang, Jiebo Luo, "Attentive Relational Networks for Mapping Images to Scene Graphs," <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, Long Beach, USA, June 2019.
					[<a href="https://arxiv.org/pdf/1811.10696.pdf">PDF</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Yixuan Zhang, Jiebo Luo, "Human-Centered Emotion Recognition in Animated GIFs with Facial Landmarks," <em>International Conference on Multimedia and Expo (<strong>ICME</strong>)</em>, Shanghai, China, July 2019.
					[<a href="https://arxiv.org/pdf/1904.12201.pdf">PDF</a>]
					[<a href="https://github.com/zyang-ur/human-centered-GIF">Data</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Yuncheng Li, Jianchao Yang, Jiebo Luo, "Action Recognition with Spatio-Temporal Visual Attention on Skeleton Image Sequences," <em>IEEE Transactions on Circuits and Systems for Video Technology (<strong>T-CSVT</strong>)</em>.
					[<a href="./projects/conv_pose_att/TCSVT-01988-2018.pdf">PDF</a>]
					[<a href="https://drive.google.com/open?id=1_W6dbGu3ykDS3PYUkGGTKbz6soxwAxh6">Data</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Yuncheng Li, Jianchao Yang, Jiebo Luo, "Action Recognition with Visual Attention on Skeleton Images," <em>International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, Beijing, China, August 2018. <red>(Oral Presentation)</red>
					[<a href="https://arxiv.org/pdf/1801.10304.pdf">PDF</a>]
					</li>	
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Yixuan Zhang, Jerry Yu, Junjie Cai, Jiebo Luo, "End-to-end Multi-Modal Multi-Task Vehicle Control for Self-Driving Cars with Visual Perceptions," <em>International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, Beijing, China, August 2018. <red>(Oral Presentation)</red>
					<a href="http://www.icpr2018.org/index.php?m=content&c=index&a=lists&catid=31%20"><em>Best Industry Related Paper Award (BIRPA)</em></a>.
					[<a href="https://arxiv.org/abs/1801.06734.pdf">PDF</a>]
					[<a href="https://youtu.be/7QGI_tmwZhw">Demo</a>]
					</li>
					<li class="gap"> <strong>Zhengyuan Yang</strong>, Wendi Cross, Jiebo Luo, "Personalized pose estimation for body language understanding," <em>International Conference on Image Processing (<strong>ICIP</strong>)</em>, Beijing, China, September 2017. <red>(Oral Presentation)</red>
					<!-- [<a href="./icip17/r4-personalized-pose.pdf">PDF</a>] -->
					</li>
				</ps>
				</td>
			</tr>
			</tbody></table>



			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Professional Experience</heading>
				</td>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./ms.jpg"  width="70" height="70">
				</td>
				
				<td width="75%" valign="center">
				<p>
					Senior Researcher,
					Microsoft, Redmond, WA <br>
					June 2021 - Current. <br>
					Research on multimodal understanding and generation.
				</p>
				</td>
			</tr>
			
			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./ms.jpg"  width="70" height="70">
				</td>
				
				<td width="75%" valign="center">
				<p>
					Research Intern, 
					Microsoft, Redmond, WA <br>
					May - Aug 2020. Advisor: <a href="https://www.linkedin.com/in/yijuan-lu-590b426/"> Yijuan Lu, 
					<a href="http://jianfengwang.me/"> Jianfeng Wang,
					<a href="https://xiyinmsu.github.io/"> Xi Yin.
					</a><br>
					Project: Text-aware pre-training for Text-VQA and Text-Caption.
				</p>
				</td>
			</tr>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./tencent/tencent.jpg"  width="120" height="50">
				</td>
				
				<td width="75%" valign="center">
				<p>
					Research Intern, 
					Tencent AI Lab, Bellevue, WA <br>
					Jan - Apr 2019. Advisor: <a href="http://boqinggong.info/"> Boqing Gong, 
					<a href="https://lwwangcse.github.io/"> Liwei Wang.
					</a><br>
					Project: Visual Grounding with Natural Language Quires.
				</p>
				</td>
			</tr>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./snap/snap.jpg"  width="70" height="70">
				</td>
				
				<td width="75%" valign="center">
				<p>
					Research Intern, 
					SnapChat, Venice, CA <br>
					May - Aug 2018. Advisor: <a href="http://www.cs.rochester.edu/~yli/"> Yuncheng Li, 
					<a href="https://sites.google.com/site/linjieyang89/"> Linjie Yang, 
					<a href="https://scholar.google.com/citations?user=DplAah0AAAAJ&hl=en"> Ning Zhang.
					</a><br>
					Project: Weakly Supervised Human Part Parsing.
				</p>
				</td>
			</tr>

			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="25%">
					<img src="./intern_saic/SAIC.png"  width="70" height="70">
				</td>
				
				<td width="75%" valign="center">
				<p>
					Research Intern, 
					SAIC Innovation Center, San Jose, CA <br>
					Jun - Aug 2017. Advisor: <a href="https://www.linkedin.com/in/jerry-yu/"> Jerry Yu. 
					</a>
					<br>
					Project: Steering Angle Control with End-to-end Neural Networks.
				</p>
				</td>
			</tr>



			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Awards</heading>
				</td>
			</tr>

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="http://www.sigmm.org/Awards/thesisaward">
					<awardtitle>2022 ACM SIGMM Award for Outstanding Ph.D. Thesis </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="https://eval.ai/web/challenges/challenge-page/906/overview">
					<awardtitle>Winner of CVPR 2021 TextCaps Challenge </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="https://referit3d.github.io/benchmarks.html">
					<awardtitle>Winner of CVPR 2021 ReferIt3D Challenge </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="https://blog.twitch.tv/en/2020/01/15/introducing-our-2020-twitch-research-fellows/">
					<awardtitle>Twitch Research Fellowship </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>

			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li> <a href="./BIRPA.JPG">
					<awardtitle>Best Industry Related Paper Award at ICPR 2018 </awardtitle><br></li> 
					</a>
				</p>
				</td>
			</tr>			
			</tbody></table>


			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Service</heading>
				</td>
			</tr>
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li><a href="https://eccv2022.ecva.net/program/outstanding-reviewers">
					<awardtitle>Outstanding Reviewer, ECCV 2022</awardtitle></a>; <a href="http://cvpr2021.thecvf.com/node/184">
					<awardtitle>Outstanding Reviewer, CVPR 2021 </awardtitle><br>
					</a></li> 
				</p>
				</td>
			</tr>			
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle><b>Exhibits and Demos Chair:</b> IEEE International Conference on Multimedia and Expo (ICME) 2024</awardtitle><br></li> 
				</p>
				</td>
			</tr>
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle><b>Area Chair:</b> ACM Multimedia Conference (ACMMM) 2024</awardtitle><br></li> 
				</p>
				</td>
			</tr>
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle><b>Senior Program Committee (SPC):</b> 37, 38th AAAI Conference on Artificial Intelligence (AAAI-23, 24)</awardtitle><br></li> 
				</p>
				</td>
			</tr>
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle><b>Associate Editor:</b> IEEE Transactions on Circuits and Systems for Video Technology (TCSVT)</awardtitle><br></li> 
				</p>
				</td>
			</tr>
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle><b>Journal Reviewer:</b> TPAMI, IJCV, TIP, TMM, TCybernetics, TCSVT, Pattern Recognition, Neurocomputing, TBioCAS, IEEE Access.</awardtitle><br></li> 
				</p>
				</td>
			</tr>
			<tbody><tr>
				<!--<td width="25%"><img src="./index_files/pacman.jpg" alt="pacman" width="160" height="160"></td>-->
				<td width="75%" valign="center">
				<p>
					<li>
					<awardtitle><b>Conference Reviewer:</b> CVPR, ICCV, ECCV, NeurIPS, ICLR, ICML, ACL, EMNLP, AAAI, ACCV, WACV, ICME, ICIP.</awardtitle><br></li> 
				</p>
				</td>
			</tr>			
			</tbody></table>


			
<!-- 			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<heading>Teaching</heading>
				</td>
			</tr>
			</tbody></table>
			<table width="100%" align="center" border="0" cellpadding="20">
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<a href="https://www.cs.rochester.edu/~gildea/2018_Spring/">
					<papertitle>TA CS246/446 - Spring 2018 </papertitle><br>
			Machine Learning
				</p>
				</td>
			</tr>
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<a href="https://www.cs.rochester.edu/courses/172/fall2017/">
					<papertitle>TA CS172 - Fall 2017 </papertitle><br>
			Datastructures and Algorithms
					</a>
				</p>
				</td>
			</tr>
			<tbody><tr>
				<td width="75%" valign="center">
				<p>
					<a href="https://www.cs.rochester.edu/~ferguson/csc/242/Spring2017/syllabus.html">
					<papertitle>TA CS242 - Spring 2017 </papertitle><br>
			Intro to Artificial Intelligence
					</a>
				</p>
				</td>
			</tr>			
			</tbody></table>

 -->
			<center>
			<!-- <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?d=1mhT__bUnjZx1JwGgr2z3jIRHGHFW7thIvYLz1NJMYY&cl=ffffff&w=a"></script> -->
			<!-- <a href="https://clustrmaps.com/site/1attr"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=1mhT__bUnjZx1JwGgr2z3jIRHGHFW7thIvYLz1NJMYY&cl=ffffff" width="100" /></a> -->
			<a href="https://clustrmaps.com/site/1bhzi"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=zcxXwQdctTsFPEw1SMscHvSL9xyUW3xNvytCTdI8loE&cl=ffffff" width="150" /></a>
			</center>

			<!-- <div style="display:inline-block;width:200px;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/5.js?i=0btq4qiwt1p&amp;m=1&amp;c=ff0000&amp;cr1=ffffff" async="async"></script></div> -->

			<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
			<tbody><tr>
				<td>
				<br>
				<p align="middle"><font size="2">
				&copy; 2024 Zhengyuan Yang. All rights reserved.
				<br>
				Template borrowed from <a href="https://people.eecs.berkeley.edu/~barron/">Jon Barron</a>. Thanks!
			<!--		  
			-->
					</font>
				</p>
				</td>
			</tr>
			</tbody></table>
<!-- 			<script type="text/javascript">
			var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
					document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
					
			</script><script src="./index_files/ga.js" type="text/javascript"></script> <script type="text/javascript">
			try {
					var pageTracker = _gat._getTracker("UA-7580334-1");
					pageTracker._trackPageview();
					} catch(err) {}
			</script> -->
		</td>
		</tr>
	</tbody></table>
	

</body></html>
